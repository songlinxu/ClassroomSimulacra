{
    "high school": {
        "lecture 1": {
            "slide 1": "WHAT IS ARTIFICIAL INTELLIGENCE. LECTURE 1",
            "slide 2": "Outline. Artificial intelligence everywhere. Logic. Statistics. Artificial neural networks.",
            "slide 3": "Artificial intelligence everywhere. chatbot on the cell phone. computers help doctors make diagnoses. some robots can understand emotions. some cars can drive without drivers. It contains four pictures that illustrate four points discussed in this slide.",
            "slide 4": "Birth of AI. 1956 Workshop at Dartmouth College. John McCarthy. Marvin Lee Minsky. Claude Elwood Shannon. -Name 'Artificial Intelligence' for the new field. It contains three pictures of the three people mentioned in this slide.",
            "slide 5": "Artificial Intelligence. According to Wikipedia AI is 'intelligence demonstrated by machines in contrast to the natural intelligence displayed by humans and other animals'. It contains a picture of the development of AI.",
            "slide 6": "Outline. Artificial intelligence everywhere. Logic. Statistics. Artificial neural networks.",
            "slide 7": "Logic. Logic: a method of using facts and rules to decide on the truth of a statement. e.g. 'Candies are sweet' and 'A lollipop is a candy' we can infer 'A lollipop is sweet'. 'All humans are mortal' and 'Socrates is a human' we can infer 'Socrates dies'. It contains a picture of an example of logic.",
            "slide 8": "Logic used in AI. necessity and probability (model logic) time (temporal logic). Deduction infer new facts from already known facts and rules. Induction learn by generating rules from known facts. store the knowledge. computer can think. Logic.",
            "slide 9": "Problems with logic-based AI. A lot of commonsense knowledge is required and it’s almost impossible to gather a sufficient amount of this knowledge. The computer must know the real-world meaning of words. Simple symbolic logic does not work well when things are uncertain. It contains two pictures of the problems with logic-based AI. One is chocolate and the other is about the weather.",
            "slide 10": "Outline. Artificial intelligence everywhere. Logic. Statistics. Artificial neural networks.",
            "slide 11": "Statistics. Statistics talks about the probability of something happening deals with uncertainties. The probability of an event is a number between 0 and 1. It contains a picture demonstrating the probability.",
            "slide 12": "Example of statistics. 'people have more headaches on rainy days'. - Bayes’ rule named after Thomas Bayes. [Can be more complex]. It contains a picture of Bayes and an example of Bayes’ rule.",
            "slide 13": "Bayes’ Rule. Bayes’ rule is stated as mathematically as the following equation: P(A|B) is a conditional probability: the probability of event A occurring given that B is true. P(A) and P(B) are the probabilities of observing A and B respectively without any given conditions. A and B must be different events. where A and B are events and P(B)≠0. It contains a picture of the formula of Bayes’ rule.",
            "slide 14": "Example. In drug testing 90% true positive results for cannabis users and 5% of people use cannabis. And 23.5% positive results of all test examples. What is the probability that a random person who tests positive is really a cannabis user? It contains a picture of the solution to the example.",
            "slide 15": "Example. It contains a picture of an example of Bayes’ rule.",
            "slide 16": "Statistics in AI. Strength (rigorous mathematical formulation). Once we reach a conclusion it’s possible to explain how you reached that conclusion and your degree of certainty about whether your conclusion is correct. Limitations (complex situations). Finding the best underlying structure is difficult in complex situations.",
            "slide 17": "Outline. Artificial intelligence everywhere. Logic. Statistics. Artificial neural networks.",
            "slide 18": "Brain. A human brain is made of 100 billion tiny cells called neurons. (1) inputs that collect signals from other neurons. (2) a decision-making section. (3) output that sends out its decision. A neuron learns by changing the connection weight to a synaptic weight. It contains two pictures of neurons.",
            "slide 19": "Artificial Neural Network. ANNs are some of the most important components of the AI that allows machine to learn from experience that is inspired by the way neurons work. Input, processing, output. It contains two picutres of neuron and neural network.",
            "slide 20": "Artificial Neural Network. Multiple artificial neurons connected to each other and become an artificial neural network. Improve the effectiveness of the ANN by changing the weight of the connection. It contains two picutres of neuron and neural network.",
            "slide 21": "Three ways of learning. Supervised learning neural networks are given training data and then compute an output which they compare to the desired output provided by the teacher. effective but requires that all input data are paired with correct answers. It contains a picture of supervised learning.",
            "slide 22": "Three ways of learning. Unsupervised learning is without a teacher the ANN is given a lot of examples and tries to figure out how data can be organized. Don’t need correct answers but not possible for everything. It contains a picture of unsupervised learning.",
            "slide 23": "Three ways of learning. Reinforcement learning is learning from a teacher who lets you try and then tells you how well you did. Takes a long time sometimes the only way. It contains a picture of reinforcement learning.",
            "slide 24": "Outline. Artificial intelligence everywhere. Logic. Statistics. Artificial neural networks.",
            "slide 25": "THANKS. Q&A. LECTURE 1",
            "slide 26": "ANN learning algorithms. These learning algorithms are rooted in a branch of physics called thermodynamics. Hopfield network. Boltzmann machine.",
            "slide 27": "Three ways of learning. Supervised learning neural networks are given training data and then compute an output which they compare to the desired output provided by the teacher. It is effective but requires that all input data are paired with correct answers. Unsupervised learning is without a teacher the ANN is given a lot of examples and tries to figure out how data can be organized. Don’t need correct answers but not possible for everything. Reinforcement learning is learning from a teacher who lets you try and then tells you how well you did. Takes a long time sometimes the only way."
        },
        "lecture 2": {
            "slide 1": "THE DEVELOPMENT OF ARTIFICIAL INTELLIGENCE. LECTURE 2",
            "slide 2": "Outline. The beginning. The first neural networks winter. The second neural networks boom. The second neural networks winter. The third neural networks boom.",
            "slide 3": "Knows the neural network of brain. The reticular theory: the nervous system was a continuous network. Camillo Golgi invented a groundbreaking method for observing the nervous system. Camillo Golgi (1843-1926). It contains two pictures of Camillo and neurons.",
            "slide 4": "Knows the neural network of brain. The neuron doctrine: postulated neurons were discrete building blocks. Santiago Ramon y Cajal made meticulous observations of the microstructure of the nervous system. Santiago Ramon y Cajal (1852-1934). Ramon y Cajal’s drawing of the cells. It contains two pictures of Santiago and neurons.",
            "slide 5": "Logic gates. Boolean function assume values from a two-element set {01}. Logic gates are physical electronic devices implementing a Boolean function. AND. OR. NOT. It contains three pictures of logic gates.",
            "slide 6": "neuron could act as a logic gate. McCulloch and Pitts thought each neuron could act as a logic gate and that it could be combined to form a complex computing system. McCulloch and Pitts. A neuron can receive inputs from other neurons, weighs and accumulates them, and decides whether or not to produce an output. It contains three pictures of McCulloch and neurons.",
            "slide 7": "How a neuron acts like a logic gate. AND/OR: assume the input and the output are OFF (0) or ON (1) also assume that all input weights are 1. NOT: assume the input weight is -1. It contains three pictures of logic gates.",
            "slide 8": "Ability to learn. Intelligent machines must be able to learn just like the human brain. Donald Hebb the first learning theory The Organization of Behavior (1949). When two neurons get excited together the connection weights increase. Frank Rosenblatt the first ANN model with learning capability perceptron in 1957. It was designed and realized as an image recognition machine",
            "slide 9": "Outline. The beginning. The first neural networks winter. The second neural networks boom. The second neural networks winter. The third neural networks boom.",
            "slide 10": "The first neural networks winter. Rosenblatt could not come up with an effective algorithm for adjusting the weights in the intermediate layer neurons. It was easy to adjust the weights to the output there were no effective ways to adjust the weights in earlier layers (hidden layers). It contains a picture of the neural network.",
            "slide 11": "Outline. The beginning. The first neural networks winter. The second neural networks boom. The second neural networks winter. The third neural networks boom.",
            "slide 12": "The second neural networks boom. Several people found ways to train multiple layer neural networks through a technique called backpropagation. ANNs with multiple layers could be trained to yield outputs closer to the desired output values. Backpropagation provides an effective way to change hidden connections to reduce output errors. It contains a picture of the backpropagation.",
            "slide 13": "Outline. The beginning. The first neural networks winter. The second neural networks boom. The second neural networks winter. The third neural networks boom.",
            "slide 14": "The second neural networks winter. Training multilayer ANNs takes a lot of computation. ANN applications were limited to small data and the results of ANNs could not compete with statistics-based algorithms. People became less interested in neural network research 1990s. It contains a picture of the AI winter.",
            "slide 15": "Outline. The beginning. The first neural networks winter. The second neural networks boom. The second neural networks winter. The third neural networks boom.",
            "slide 16": "Computing speed accelerated (time!). Graphics processing units (GPUs) accelerated the speed of the computing that ANNs required. Moore’s law states that the number of transistors on computer chips doubles every two years. It contains a picture of the GPU.",
            "slide 17": "Current state of ANNs. Interpretability. It’s difficult to interpret what ANNs are doing raises safety concerns. Generalizability. ANNs don’t generalize when they encounter inputs different from those they are trained on. It contains a picture of autonomous driving.",
            "slide 18": "Summary screenshot.",
            "slide 19": "THANKS. Q&A. LECTURE 2"
        },
        "lecture 3": {
            "slide 1": "ARTIFICIAL NEURAL NETWORKS THAT LEARN FROM TEACHERS. LECTURE 3",
            "slide 2": "Outline. Supervised learning: learning from a teacher. Computing the output. Comparing the output to the desired output. Changing the connection weight. Backpropagation.",
            "slide 3": "Supervised learning. A supervised learning ANN learns from a teacher. After the network is given the correct answer it can then adjust its connection weights to arrive at correct answers. It contains two pictures of learning and neural network.",
            "slide 4": "A simple network (more details). Has two artificial neurons (green stars in the figure) specialized in finding the letter x and the symbol +. One-layer network. It contains a picture of the neural network.",
            "slide 5": "The delta rule. Delta: symbolizes a change or difference. Uppercase: Δ and lowercase: 𝛿. The delta rule for linear artificial neuron is. It contains a picture of formula.",
            "slide 6": "Outline. Supervised learning: learning from a teacher. Computing the output. Comparing the output to the desired output. Changing the connection weight. Backpropagation.",
            "slide 7": "Computing the output. Our inputs are: 'x' and '+'. Assume that the weights are all 0.01. Then the weight pattern of the artificial neurons would look like. It contains three pictures of the input, weight, and output.",
            "slide 8": "Computing the output. Output for x = sum of (input * weight) for each input plate ...",
            "slide 9": "Outline. Supervised learning: learning from a teacher. Computing the output. Comparing the output to the desired output. Changing the connection weight. Backpropagation.",
            "slide 10": "Comparing the output to the desired output. Output: 0.05 Target: 1. It contains a picture of an example.",
            "slide 11": "Outline. Supervised learning: learning from a teacher. Computing the output. Comparing the output to the desired output. Changing the connection weight. Backpropagation.",
            "slide 12": "Changing the connection weight. The delta rule states that the change of weight depends on both the input and the desired change. Learning constant: if it is smaller than 1 adjust the weight little by little. It contains a table describing the input and weight change.",
            "slide 13": "Example: Weight change for the 'x' neuron. It contains an example of weight change.",
            "slide 14": "Example 2: Weight change for the 'x' neuron. For input plates whose values are 0. It contains an example of weight change when inputs are 0.",
            "slide 15": "Example 3: Weight change for the 'x' neuron. After applying the delta rule the weight pattern of the 'x' artificial neuron will be. It contains a picture of calculation.",
            "slide 16": "Summary. ANNs work by training the network for samples 'x' and '+' repeatedly. The output of the artificial neuron will become closer and closer to the desired output. It contains a table of relationship between input and neuron.",
            "slide 17": "Outline. Supervised learning: learning from a teacher. Computing the output. Comparing the output to the desired output. Changing the connection weight. Backpropagation.",
            "slide 18": "Backpropagation. Backpropagation was developed to train incoming connection weights to hidden layers. It provides a method to train multilayer neural networks.",
            "slide 19": "Backpropagation. Extends the delta rule in two steps: change the way neurons compute their outputs. add a rule for error backpropagation. It contains a picture of the backpropagation.",
            "slide 20": "Change in the Computation. The delta rule: a neuron computes 'the weighted sum of inputs' is extended as: a neuron computes some math transformation of the weighted sum. The weighted sum is also called as inner state because it represents some information about the network.",
            "slide 21": "Change in the Computation. Rectified Linear Unit (ReLU) is one of the commonly used transformations. Given an inner state Mimics the way how biological neurons are triggered!",
            "slide 22": "Change in the Computation. The output is 0.05 (not changed by ReLU because it is > 0). The desired output change: 0.95 Error: the deviation of the actual output from the desired output. (the desired change)",
            "slide 23": "Change in the Computation. contribution ratio: the ratio between the inner state change and the output change. the weight change proportional to: input * contribution ratio * error = 1*1*0.95",
            "slide 24": "Change in the Computation. learning constant: 0.01. It contains a picture of the calculation.",
            "slide 25": "Error back propagation. Backpropagation: the propagation of error from the output layer to the middle layer before it (Backwards!)",
            "slide 26": "Summary. Supervised learning: learning from a teacher. Computing the output. Comparing the output to the desired output. Changing the connection weight. Backpropagation.",
            "slide 27": "THANKS. Q&A. LECTURE 3"
        },
        "lecture 4": {
            "slide 1": "Algorithms that learn from teachers. Lecture 4",
            "slide 2": "Outline. What is a decision tree. How to build a decision tree. K-Nearest Neighbors (KNN). The Effect of K",
            "slide 3": "How do humans classify. Humans use objects’ characteristics to classify objects resulting in the label of objects.",
            "slide 4": "What is a decision tree. A decision tree is a flowchart-like diagram that shows the various outcomes from a series of decisions. It can be used as a decision-making tool for research analysis or for planning strategy. It contains a picture of decision tree.",
            "slide 5": "The structure of a decision tree. Internal node. Branch. Leaf node. It contains a picture of the structure of a decision tree.",
            "slide 6": "More details of a decision tree. In the prediction a certain attribute is used to make a judgment at the internal nodes of the tree. According to the judgment result which branch to enter is decided. When it reaches the leaf node the classification result is obtained. It contains a picture of the decision tree.",
            "slide 7": "An example of classifying fruits. We find a most useful feature such as shape: curve or round. Split the dataset: If the fruit is curved then it is a banana. Select the next best feature until each node contains only one class. At last we build a decision tree. The picture contains an example of decision tree.",
            "slide 8": "Outline. What is a decision tree. How to build a decision tree. K-Nearest Neighbors (KNN). The Effect of K",
            "slide 9": "How to build a decision tree? When we face many features it is hard for us to build tree by hand. Can we teach machine to find it? How to find the most useful feature on each node? When should we stop growing the tree? It contains an example of decision tree table.",
            "slide 10": "Feature selection. Which one is better? The left. Feature selection is to screen out the features with high correlation with the classification results or the features with strong classification ability. It contains a picture of feature selection.",
            "slide 11": "Outline. What is a decision tree. How to build a decision tree. K-Nearest Neighbors (KNN). The Effect of K",
            "slide 12": "Machine Learning: Geometric View. We can view examples as points in a d-dimensional space where d is the number of features. Assumption: Closer points in feature space have similar semantics. It contains a picture of feature space",
            "slide 13": "I-Nearest-Neighbor (1NN). To classify a new example x: Label x with the label of the closest example to x in the training set. Closest to red class. Euclidean Distance. It contains a picture of 1NN.",
            "slide 14": "K-Nearest-Neighbors (KNN). x is closest to a red point. But most of the next closest points are blue. If we only consider the closest point the result won’t be accurate enough. It contains a picture of KNN.",
            "slide 15": "K-Nearest-Neighbors (KNN). K-Nearest-Neighbor (KNN): Find k nearest neighbors of x. Label x with the majority label within the k nearest neighbors. Consider: How can we handle ties for even values of k? It contains a picture of KNN.",
            "slide 16": "Outline. What is a decision tree. How to build a decision tree. K-Nearest Neighbors (KNN). The Effect of K.",
            "slide 17": "The effect of K. Increasing k simplifies the decision boundary. Majority voting means less emphasis on individual points. Fit Noise. Smooth Boundary. I-Nearest-Neighbor. I5-Nearest-Neighbor. It contains a picture of the effect of K.",
            "slide 18": "The Effect of K. Choose best k on the validation set. (Often set k as an odd number). It contains a graph of choosing the best k.",
            "slide 19": "KNN Summary. Advantages: Data number can be very large. Class number can be very large. All other ML algorithms may fail here! Disadvantages: Slow at inference time. Fooled easily by irrelevant attributes",
            "slide 20": "Thanks. Q&A. Lecture 4"
        },
        "lecture 5": {
            "slide 1": "ARTIFICIAL NEURAL NETWORKS THAT LEARN FROM EXAMPLES (I). Lecture 5",
            "slide 2": "Outline. Unsupervised learning: Learning without a teacher. Unsupervised learning: How to choose features. Unsupervised ANNs that make maps. How a self-organizing features map learns.",
            "slide 3": "Unsupervised learning. Supervised Learning: Requires a teacher to tell the ANN the correct answer. Unsupervised Learning: Learns by examples without the correct answer. E.g. ANNs are given images of apple and images of orange without being told what the fruit is. ANN learns to group them into apples and oranges. It contains a picture of an example of unsupervised learning.",
            "slide 4": "Other Application Scenarios. Unusual bank users found. Some people use bank accounts to commit illegal acts. It is difficult to find out these bank users through human analysis. Unsupervised learning helps us to quickly classify behaviors. We can quickly exclude normal users and conduct in-depth analysis of abnormal behaviors in a more targeted manner.",
            "slide 5": "Other Application Scenarios. User segmentation for advertising. The more targeted advertising can be the better the effect will be. Through unsupervised learning we can not only segment users according to dimensions such as gender, age, and geographic location, but also based on user behavior.",
            "slide 6": "Outline. Unsupervised learning: Learning without a teacher. Unsupervised learning: How to choose features. Unsupervised ANNs that make maps. How a self-organizing features map learns.",
            "slide 7": "Features. Why this grouping is feasible? ---apples (oranges) are similar to each other. Features: Characteristics of the objects selected to make all the inputs widely different from each other. ANN has to pay attention to useful features! Useful (important): colors, textures. Unreliable: round, shiny.",
            "slide 8": "Features. What should good features look like? The samples have to be quite different in this feature. E.g. color (useful) round (unreliable). The feature that is highly correlated with the target sample. E.g. color (useful) ear shape (unreliable).",
            "slide 9": "Outline. Unsupervised learning: Learning without a teacher. Unsupervised learning: How to choose features. Unsupervised ANNs that make maps. How a self-organizing features map learns.",
            "slide 10": "Self-Organizing Feature Map (SOFM). SOFM: an unsupervised learning ANN model. Invented by Teuvo Kohonen, a Finnish researcher in engineering. A tool for mapping complex information on a sheet. It contains a picture of Teuvo.",
            "slide 11": "Self-Organizing Feature Map (SOFM). SOFM learns to create a neural map: Similar inputs are placed in nearby locations. Inputs presented more frequently occupy more areas. The more a specific type of input is seen, the more minute differences among the inputs belonging to that type are represented in the map. It contains a picture of SOFM.",
            "slide 12": "Example of Self-Organized Structure. Primary sensory area: Two-dimensional map of the body preserves the topology of the body similar to the map of an SOFM. The relative area size of each part of the body is determined by how much that part is used.",
            "slide 13": "Example of Self-Organized Structure. Primary Visual Cortex: (also in the brain). David Hubel and Torsten Wiesel: primary visual cortex containing a subgroup of neurons sensitive to a particular orientation. Experiment: If one of a young cat’s eyes is kept shut, the formation of the primary visual cortex is disrupted. The development of the primary visual cortex involves a self-organizing process. It contains two pictures of the primary visual cortex.",
            "slide 14": "Outline. Unsupervised learning: Learning without a teacher. Unsupervised learning: How to choose features. Unsupervised ANNs that make maps. How a self-organizing features map learns.",
            "slide 15": "How an SOFM learns. SOFM learns by being exposed to input samples. Similar inputs activate nearby neurons, the neighborhood shrinks as the network learns more samples. The inputs are the same size: the sum of the squares of numbers in each input data is 1. E.g. (0.2 -0.1 0.5 0.2 0.69). The picture contains an example of SOFM.",
            "slide 16": "How an SOFM learns. Step 1. Set the weights of each neuron (dots on the plane as the right figure shows) to small random values. E.g. [0.2 -0.1 0.5 0.2 0.69] [-0.5 0.5 0.3 0.2 -0.61] and so on. Step 2. All neurons get activated by a new input pattern. Compute the activations by taking the weighted sum of the input. E.g. if the input pattern is [0.2 -0.1 0.3 0.2 0.91] then the neuron with weight [0.2 -0.1 0.5 0.2 0.69]. Activation will be: 0.2*0.2 + (-0.1)*(-0.1) + 0.5*0.3 +0.2*0.2 +0.69*0.91 = 0.868. The picture contains an example of SOFM.",
            "slide 17": "How an SOFM learns. Step 3. Choose the neuron with the largest activation and define the neighborhood within a radius 'r' from that neuron. E.g. the neuron with weight [0.2 -0.1 0.5 0.2 0.69] wins because the activation is bigger and its connection pattern is closer to the input pattern. Step 4. Modify the weights of all the neurons in the neighborhood so the weights yield larger activations. The picture contains an example of SOFM.",
            "slide 18": "How an SOFM learns. Step 5. Slightly reduce the neighborhood size and connection weight modification rate. Step 6. Go back to step 2. Repeat. The picture contains an example of SOFM.",
            "slide 19": "Some notes for SOFM. Initially the weights are random, the neighborhood covers the whole neural plane. The input weights to all neurons will be set to the average of all the input patterns. When the neighborhood area starts to shrink, some specialization of different areas occurs.",
            "slide 20": "Outline. Unsupervised learning: learning without a teacher. Unsupervised learning: how to choose features. Unsupervised ANNs that make maps. How a self-organizing features map learns.",
            "slide 21": "THANK YOU! Q&A. Lecture 5"
        },
        "lecture 6": {
            "slide 1": "ARTIFICIAL NEURAL NETWORKS THAT LEARN FROM EXAMPLES (II). Lecture 6",
            "slide 2": "Outline. Unsupervised ANNs that make groups. Adaptive resonance theory 1. K-means Clustering. Unsupervised ANNs that compress.",
            "slide 3": "Grouping. An unsupervised learning neural network can group similar inputs. Grouping is also called clustering classification or categorization. E.g., grouping images of cats and images of dogs. The picture contains an example of grouping.",
            "slide 4": "Adaptive resonance theory 1 (ART1). Adaptive resonance theory 1: an example of a neural network that performs grouping. Created by cognitive scientist Stephen Grossberg and mathematician Gail Carpenter. It contains a picture of Stephen.",
            "slide 5": "Adaptive resonance theory 1 (ART1). ART1 can form stable memories by creating 'resonance' between two flows of data. Bottom-up activation from the input. Top-down activation from the memory. To make this happen an ART network has three layers: input (bottom) interface (middle) category (top). It contains a picture of the structure of ART1.",
            "slide 6": "Details about ART1. Input layer: sends signals to the interface layer through one-to-one connections. Interface layer: activates the category layer through full connections. Category layer: sends back activations to the interface layer through full connections. The interaction between the interface and category layer weights creates resonance. It contains a picture of the structure of ART1.",
            "slide 7": "Outline. Unsupervised ANNs that make groups. Adaptive resonance theory 1. K-means Clustering. Unsupervised ANNs that compress.",
            "slide 8": "K-means Clustering. K-means is the most commonly used clustering method. The input is a sample set (point set). The samples with similar characteristics will be clustered into one category. Most of the time the examples will be abstracted into points on the plane. It contains a picture of an example of k-means.",
            "slide 9": "The process of k-means clustering. Step 1: select the number k of categories to be clustered and select k center points. E.g., k=3. The k center points are randomly selected on the plane. It contains a picture of an example of step 1.",
            "slide 10": "The process of k-means clustering. Step 2: For each sample point find the center point closest to it and the points closest to the same center point is a class. Thus completing a clustering. Closest to green! Closest to red! Closest to blue! It contains a picture of an example of step 2.",
            "slide 11": "The process of k-means clustering. Step 3: determine whether the categories of the samples points before and after the clustering are the same. If they are the same, the algorithm terminates. Otherwise, enter step 4. In this example the sample points before and after are different then we enter step 4. It contains a picture of an example of step 3.",
            "slide 12": "The process of k-means clustering. Step 4: for the sample points in each category calculate the center point of these sample points as the new center points. Continue to step 2. It contains a picture of an example of step 4.",
            "slide 13": "The process of k-means clustering. The remaining process of the example: step 2 -> step 3 -> step 4 -> step 2 -> step 3 -> end. It contains a picture of an example of K-means.",
            "slide 14": "The Cycle of k-means. Assign sample point to the closest center point. Update the center point for each category.",
            "slide 15": "Outline. Unsupervised ANNs that make groups. Adaptive resonance theory 1. K-means Clustering. Unsupervised ANNs that compress.",
            "slide 16": "Unsupervised ANNs that compress. Unsupervised Work: gives the supervised ANNs inputs only ---teaching a supervised ANN to reproduce inputs. These networks usually have a middle layer that has a much smaller number of neurons making an hourglass-structure. It contains a picture of the structure of unsupervised ANNs.",
            "slide 17": "Unsupervised ANNs that compress. This network can transform the input pattern to the middle layer pattern. Autoencoders: the networks encodes a big input pattern to a small 'code' that represents the input. Usage: remove noises. It contains a picture of the structure of unsupervised ANNs.",
            "slide 18": "Image compression with k-means. In the problem of image compression K-means clustering will group similar colors together into k clusters of different colors. The larger the k is, the closer the compressed image is close to the original image. It contains a picture of an example of image compression.",
            "slide 19": "Summary. Unsupervised ANNs that make groups. Adaptive resonance theory 1. K-means Clustering. Unsupervised ANNs that compress.",
            "slide 20": "THANK YOU! Q&A. Lecture 6"
        },
        "lecture 7": {
            "slide 1": "ALGORITHMS THAT LEARN THROUGH TRIAL AND ERROR (I). Lecture 7",
            "slide 2": "Outline. Reinforcement learning: the third way (highlighted). Learning the situation(V-learning)",
            "slide 3": "Reinforcement learning: the third way. Supervised learning: Learn from teachers. Unsupervised learning: Learn from examples. Pictures that are related to learning algorithms.",
            "slide 4": "The limitation of supervised / unsupervised learning. Context: A baby is learning to walk. Supervised learning. Tell the baby which muscles should be used? A picture of big red X besides supervised learning. Unsupervised learning. Observe sensory inputs or feedback from the feet and legs alone? A picture of big red X besides unsupervised learning. Randomly generate movements and remember how they work in each situation. A picture of big green check mark besides the last sentence. A picture of a crawling baby.",
            "slide 5": "Reinforcement learning. Reinforcement: When you try something and then get rewarded, you will increase the chance of repeating the action. Reinforcement learning: Operating in a dynamic environment and learning from collected experiences to act correctly. A picture of an agent doing action to the environment and environment returns rewards and observations to the agent.",
            "slide 6": "History of reinforcement learning. Reinforcement learning was originally a concept used in behavioral psychology. Richard Sutton, a Canadian computer scientist, is one of the pioneers in the development of reinforcement learning in the context of AI. He contributed to a RL method called: Temporal Difference learning. A picture of Richard Sutton.",
            "slide 7": "Key conponents of reinforcement learning. A picture of an agent doing action to the environment and environment returns rewards and observations to the agent where the agent is a deep learning model and the environment is the world. Goal: Get most rewards! A picture of a bag of money besides the last sentence.",
            "slide 8": "An example of reinforcement learning. A mouse tries to get some cheese located at the end of a maze. Agent: learner. The mouse. Environment: The maze. Action: agent produces an output. Turning/running. State: the situation of the world observable by the agent. Location in the maze. Reward: cheese. A picture that illustrate this example of reinforcement learning.",
            "slide 9": "An example of reinforcement learning.  agent performs an action -> state changes  -> action depends on the state -> the agent and the state form a loop. The reward is used to change the strategy for choosing the next action, but it is not usually used to decide the next action directly. A picture that illustrates the 'A mouse tries to get some cheese located at the end of a maze' reinforcement learning example.",
            "slide 10": "Train machines to play games. The machine makes moves and learns how much the situation improves or worsens. Improved situations: more points which means that it will eventually win. A picture of two player Pong game.",
            "slide 11": "AlphaGo. AlphaGo was developed by Google’s DeepMind team. Employed reinforcement learning. Beat Lee Sedol, an 18-time Go world champion in 2016. AI outperforming humans at a task using reinforcement learning. A picture of AlphaGo playing Go with Lee Sedol.",
            "slide 12": "Robot Control. Steps: Chooses an action. Receives the evaluation. Improves the selection of the next actions. Fits the model of reinforcement learning! A picture of a configurable robot with modular legs.",
            "slide 13": "Outline. Reinforcement learning: the third way. Learning the situation (V-learning) (highlighted).",
            "slide 14": "Value learning (V-learning). Value learning places a value on the situation. Learning how good or bad the situation is can be important. Value of a situation: The total reward that an agent can expect to collect from that state and onwards into the future. Two animations showing two different total rewards a agent could get by choosing different actions.",
            "slide 15": "The details of V-learning. The system learns to determine the value of a situation. We say the situation is s. e.g. The possible positions of the mouse. We say the current reward is r. reward means the evaluation of some situation, e.g., the cheese. Goal: for every situation s, find the value V. A picture showing a mouse going through a maze where each cell is a state.",
            "slide 16": "The details of V-learning. A simple situation: The current reward is the only indicator of the value in the current situation. state = s. reward = r. Value of s: V_s = r. A table of that has a row of states and another row of value that correspond to each state. A picture showing a mouse going through a maze where each cell is a state.",
            "slide 17": "The details of V-learning. A simple situation: The current reward is the only indicator of the value in the current situation. However, r is not always consistent in real world. new state = s. reward = r. Value of s: V = V_old + α * (r - V_old). Averaging the old value with new reward. A picture showing a mouse going through a maze where each cell is a state.",
            "slide 18": "The value function considering future effect. A complex situation: The total reward that an agent can expect to collect from that state and onwards into the future. In Go, we don’t know the outcome until the agent clearly wins. This holds for many board games. In practice, we also consider the effect after the current action. A picture of Go.",
            "slide 19": "The value function considering future effect. Values: g: the degree of the effect of the future situation. (0<g<1, we assume 0.5). V_next : the value of the best possibe situation next (s’). Note: V_next is looked up from the current table. updated value: V_new = V_old + α *((r + g*V_next) - V_old). Note: we use r + 0.5*V_next instead of r. A picture showing a mouse going through a maze where each cell is a state and it illustrates the formula.",
            "slide 20": "Neural network: a more complicated situation. A few states: (3 * 6 = 18 states in the mouse-maze example). We only need a table that stores the average evaluation for each state. Large members of states: (chess has possibly 7.7*10^45 states/arrangments). We need a neural network.We train a neural network to output r (reward) when the input is s (state). A picture of chess.",
            "slide 21": "Outline. Reinforcement learning: the third way. Learning the situation (V-learning). A picture of an agent doing action to the environment and environment returns rewards and observations to the agent where the agent is a deep learning model and the environment is the world. A table of 3 columns: Type, Value of given state s, Condition. The first row of that table is V-learning, r, The current reward is the only indicator of the value. The second row of that table is V-learning, V_old + α * (r - V_old), r is not consistent. The third row of that table is V-learning, V_old + α * ((r + g * V_next) - V_old), 'Considering future effect. Be smart.' The last row of that table is V-learning, Neural network, # states is out of hand.",
            "slide 22": "Q&A. Lecture 7.",
            "slide 23": "An classic example of V-learning. A neural pole balancing demonstrated on a computer. states: position and the speed, infinite. action: movement of the palm. reward: we get rewards when the pole is closer to upright. A picture illustrating the experiment.",
            "slide 24": "Q-learning. We learn how good it is to take a specific action a in the situation s. e.g. s is the arrangement of pieces on a chessboard, a is a possible move. We can look up the resultant evaluation r in a lookup table. The table can be used to locate the best action by looking through the entries with the current situation s. A table with different actions being the columns, different states being the actions, and each cell being the reward corresponding to that state and action.",
            "slide 25": "The details of Q-learning. We can smooth out different evaluations of the same situation-action pair as in V-learning. action that would result in the best next state = a’. the best next state = s’. Q function of a and s: Q_new = Q_old + 0.1*(r-Q_old). A picture of a real life digital mouse trying to find cheese in a maze. A table with different actions being the columns, different states being the actions, and each cell being the reward corresponding to that state and action which is Q_old + 0.1*(r-Q_old).",
            "slide 26": "Outline. Reinforcement learning: the third way. Learning the situation (V-learning). Learning to act in the situation (Q-learning). Autonomous cars (highlighted).",
            "slide 27": "Autonomous cars. Also called self-driving cars. Methods: 1. They use cameras and other sensors to assess driving situations. 2. The program is trained using recorded scenes and stimulators. A picture of one of Google’s Waymo self-driving cars.",
            "slide 28": "Autonomous Cars. Pros and cons: 1. Autonomous cars can maneuver in tricky situations. 2. Dealing with cars driven by humans is difficult. Issues: 1. Responsible for crashes. 2. Privacy issues, including the potentiality for mass surveillance. A picture of One of Uber’s experimental self-driving cars.",
            "slide 29": "Summary. Reinforcement learning: the third way. Learning the situation (V-learning). Learning to act in the situation (Q-learning). Autonomous cars."
        },
        "lecture 8": {
            "slide 1": "ALGORITHMS THAT LEARN THROUGH TRIAL AND ERROR (II). Lecture 8.",
            "slide 2": "Outline. Learning to act in the situation (Q-learning) (highlighted). Autonomous cars.",
            "slide 3": "Quick review: reinforcement learning and V-learning. A picture showing 3 types of machine learning: unsupervised, supervised, and reinforcement. A picture of an agent doing action to the environment and environment returns rewards and observations to the agent. Reward: the instantaneous benefit of being in a specific state, Value: the total reward that an agent can expect to collect from that state and onwards into the future. V-learning learns the value for different situations, i.e. how good the situation is.",
            "slide 4": "Q-learning. V-learning: how good it is for the agent to be in a given state. Q-learning: how good it is to perform a given action in a given state. Breakdown of the interaction between the agent and the environment: Agent collects the initial state S0 of the robot from the environment. Based on S0 state, take action A0. Get reward R1 from the environment. Transition to the new evironment---state S1. A: action  S: state  R: reward. A picture of a robot crossing a thin bridge.",
            "slide 5": "Q-learning. Q-learning algorithm takes as input a series of [state, action, reward]. The ultimate goal of agent is to maximize the overall reward. {S0, A0, R1, S1, A1, R2, S2, ...}. Agent is in state S0 and takes action A0, which causes it to get the reward R1 and be in state S1;It performs A1, receives the reward R2, enters state S2. A picture showing a mouse going through a maze where each cell is a state.",
            "slide 6": "The steps of Q-Learning. Initialize Q table. Choose an action a. Perform action. Measure reward. Update Q. Go back to Choose an action a. At the end of the training: Good Q* table. Q-table is a table that records Q-values. Q-value: the maximum expectation of long-term rewards for different actions taken in each state. Value: the total reward that an agent can expect to collect from that state and onwards into the future.",
            "slide 7": "Q-learning details. A simple situation: The current reward is the only indicator of the value in the current situation. A picture of a cheese at cell (2,1) in a 3x6 maze. A table of with entries corresponding to action/state where the actions are going up/down/left/right and the entries corresponding to state (1,1) and (3,1) are all 0s.",
            "slide 8": "Q-learning details. A simple situation: The current reward is the only indicator of the value in the current situation. A picture of a cheese at cell (2,1) in a 3x6 maze. A table of with entries corresponding to action/state where the actions are going up/down/left/right with the entries corresponding to state (1,1) being -1,+100,N/A,-1 and N/A,-1,N/A,+100 for state (3,1).",
            "slide 9": "Q-learning details. However, r is not always consistent in real world. We can smooth out different evaluations of the same situation-action pair as in V-learning. Action that would result in the best next state = a’. Best next state = s’. Q function of a’ and s’: Q_new = Q_old + α*(r-Q_old). A picture of a real life digital mouse trying to find cheese in a maze. A table with different actions being the columns, different states being the actions, and each cell being the reward corresponding to that state and action which is Q_old + 0.1*(r-Q_old).",
            "slide 10": "The details of Q-learning (continued). We can extend Q-learning to consider the future. Modified reward: r + g*Q_next. New Q-value: Q_new = Q_old + α* ((r + g*Q_next) - Q_old). Note: Future effect g is assumed as 0.5. Q_next is the table entry for the best-possible next situation s’ and the action a’ that will bring about that state. A picture showing a mouse going through a maze where each cell is a state.",
            "slide 11": "A missing point: how to choose action? Exploration: Explore new area in the environment. Exploitation: Collect the most reward you already know about. Two animations showing two different total rewards a agent could get by choosing different actions.",
            "slide 12": "Summary. Advantages: Model free of the environment. Disadvantages: In the case of a large number of states. The Q-table will be large and searching and storage will consume a lot of time and space.",
            "slide 13": "Outline. Learning to act in the situation (Q-learning). Autonomous cars (highlighted).",
            "slide 14": "Definition. An autonomous car is a vehicle capable of sensing its environment and operating without human involvement. An autonomous car can go anywhere a traditional car goes and do everything that an experienced human driver does. A picture of the interior of a self-driving car.",
            "slide 15": "6 levels of driving automation. The Society of Automotive Engineers (SAE) currently defines 6 levels of drving automation ranging from Level 0 (fully manual) to Level 5 (fully autonomous). Level 0-2: The human monitors the driving environment. Level 3-5: The automated system monitors the driving environment. A picture of the 6 levels of driving automation.",
            "slide 16": "Under the framework of reinforcement learning. Agent: The car or the computer inside the car. Environment: Many components. Traffic lights, pedestrian, other cars, road signs... State: Current location, green/red light, position of pedstrain, speed limit... Action: Brake, accelerate, turn left... Reward: No accident. A picture of an agent doing action to the environment and environment returns rewards and observations to the agent where the agent is the car and the environment is the world.",
            "slide 17": "How do autonumous cars work? Autonomous cars create and maintain a map of their surroundings based on a variety of sensors situated in different parts of the vehicle. Radar sensors monitor the position of nearby vehicles. Video cameras detect traffic lights, .etc. A picture of a radar. A picture of a camera.",
            "slide 18": "How do autonumous cars work? Lidar (light detection and ranging) sensors bounce pulses of light off the car’s surroundings to measure distances. Ultrasonic sensors in the wheels detect curbs and other vehicles when parking. A picture of lidar. A picture of Ultrasonic sensors.",
            "slide 19": "Challenges with autonomous cars. Lidar and Radar: Lidar is expensive. Multiple lidar signals may interfere with one another. Weather Conditions: How will the cameras and sensors track lane markings if the  markings are obscured by water, oil, ice, or debris? A picture of an icy road at night. A picture of a foggy road.",
            "slide 20": "Challenges with autonomous cars. Accident Liability: Who is liable for accidents caused by an autonomous car? Artificial vs. Emotional Intelligence: Human drivers rely on subtle cues and non-verbal communication to make split-second judgment calls and predict behaviors. A picture of a car crash.",
            "slide 21": "Benefits of autonomous cars. Reduce traffic congestion. Cut transportation costs by 40%. Improve walkability and livability. Free up parking lots for other uses. A picture of a self-driving taxi.",
            "slide 22": "Summary. The program can assess driving situations using cameras and other sensors. Methods: The program is trained using recorded scenes and stimulators. Pros and cons: 1. Autonomous cars maneuver in tricky situations. 2. Dealing with cars driven by humans. A picture of One of Google’s self-driving cars.",
            "slide 23": "RL lectures in a slide. A table with the following content (| indicates next column, > indicates next row). Type|Value of given state s|Condition>V-learning|r|The current reward is the only indicator of the value>V-learning|V_old + α * (r - V_old)|r is not consistent>V-learning|V_old + α * ((r + g * V_next) - V_old)|Considering future effect. Be smart.>V-learning|Neural network|# states is out of hand>Type|Q-value of given state s and action a|Condition>Q-learning|r|The current reward is the only indicator of the value>Q-learning|Q_old + α * (r - Q_old)|r is not consistent>Q-learning|Q_old + α * ((r + g * Q_next) - Q_old)|Considering future effect. Be smart.>Q-learning|Neural network|# states is out of hand",
            "slide 24": "Thanks!Q&A. Lecture 8.",
            "slide 25": "Q-table. We learn how good it is to take a specific action a in the situation s. e.g. s is the arrangement of pieces on a chessboard, a is a possible move. We can look up the resultant evaluation r in a lookup table. The table can be used to locate the best action by looking through the entries with the current situation s. A table of rewards corresponding to action/state."
        },
        "lecture 9": {
            "slide 1": "TALKING ARTIFICIAL NEURAL NETWORKS. Lecture 9",
            "slide 2": "Outline. How Do Voice-Controlled Personal Assistants Work?(highlighted). Speech Recognition. Sequence Processing. Task Execution.",
            "slide 3": "How Do Voice-Controlled Personal Assistants Work? We can ask for a joke to our voice-controlled personal assistant, and it seems to know what we’re talking about. Two pictures of Alexa, a personal assistant.",
            "slide 4": "How Do Voice-Controlled Personal Assistants Work? AI is behind those responses, tackling the very difficult tasks of understanding your speech, judging how to respond, and acting or answering using a natural-sounding voice. Voice recognition. Linguistic analysis. Conversational control. Database. Web content retrieval. A picture of a personal assistant.",
            "slide 5": "Outline. How Do Voice-Controlled Personal Assistants Work?. Speech Recognition (highlighted). Sequence Processing. Task Execution.",
            "slide 6": "Recognition Accuracy. We first need a system to recognize speech when creating a voice-controlled personal assistant. The accuracy of speech recognition is very demanding. 95% is not good. 99% is good. A picture of The Telephone Game to show the importance of speech recongition accuracy.",
            "slide 7": "ANN Has Contributed a Lot to Voice Recognition. A speech-recognition system does what our ears do: Converts speech sounds, A continuous change in air pressure, A continuous change in frequencies. A picture of a tuning fork.",
            "slide 8": "Speech Recognition. Here we talk about harmonies, a mixture of frequencies at each point. An ANN can translate frequencies into a series of linguistic sounds, such as /a/ /i/ /u/ /e/ /o/. Series of sounds then can be converted into words and phrases. A picture of a heatmap of frequency vs time.",
            "slide 9": "Outline. How Do Voice-Controlled Personal Assistants Work?. Speech Recognition. Sequence Processing (highlighted). Task Execution.",
            "slide 10": "How Should the Words be Represented in an ANN? Question: Why not type the words into a computer as a string of the alphabet for input? Assigning different neurons to different words. Then, add more information relating to each word: Represent a word with a group of words the original word is often used with. E.g. 'Cats' with 'meow' 'walk'. A picture of word cloud. A picture of a word connected to other words that it is related to.",
            "slide 11": "Words Representation. Method: finding out how often the word appears after which words and before which words by going through a huge data set. E.g. Words: {Cats, Dogs, Bark, Meow, Eat}. Phrases: {'Cats meow,' 'Dogs bark,' 'Cats eat,' 'Dogs eat'}. Result:'Cats' is often before 'meow' and 'eat', which can be expressed as (0,0,0,1,1). 'Cats' doesn’t often appear after any words, which can be expressed as (0,0,0,0,0). 'Cat” = (0,0,0,0,0, 0,0,0,1,1). A table illustrating this method.",
            "slide 12": "More Models of Word Representation. Bag-of-Words Model: Taking neighboring words in a wider scope without considering the exact word order. Representation of words is called word embeddings. A picture showing the idea of word embedding where similar words have closer Euclidean distance.",
            "slide 13": "The Next Difficulty: Time. When an ANN works on a sentence, it needs to remember the previous words because the order is important. 'I am hungry' and 'Am I hungry' are different. The network remembers the word it has seen by sending the hidden layer activation back to the right input. A picture illustrating the last sentence.",
            "slide 14": "Recurrent Neural Network. The network is called a Recurrent Neural Network (RNN) because it has a loop in the connection to feed the previous activations back to the input. This network receives one word at a time and is trained to generate an output each time using supervised learning with additional 'desired outputs'. Another usage: They can also be trained to represent phrases and sentences. A picture of RNN.",
            "slide 15": "Training an RNN. Training an RNN by using backpropagation is more complicated than training an ANN with no loops. Unfolding: The loops are removed by duplicating parts of the neural network whenever they represent different parts of the sentence. After unfolding, we can train the RNN the way we train ANNs without loops. A picture illustrating the concept of unfolding.",
            "slide 16": "The Long Short-Term Memory (LSTM) Model. Humans are thought to have two kinds of memory: Short-Term Memory and Long-Term Memory. E.g. 'Memorizing a phone number to make a phone call' uses your Short-Term Memory. 'Memorizing your friend's name' uses your Long-Term Memory. A picture showing repitition of short-term memory could convert to long-term memory.",
            "slide 17": "The Long Short-Term Memory (LSTM) Model. In neural networks: memories stored related to the connection strength between neurons are called long-term memories, and memories stored related to the activity pattern of the neurons are called short-term memories. A picture showing the process of sensory input becoming sensory memory becoming short-term memory becoming long-term memory.",
            "slide 18": "The Long Short-Term Memory (LSTM) Model. The short-term memory in the LSTM can be kept unchanged as several inputs are processed. Short-Term Memory of LSTM: can be kept or rewritten, depending on the current short-term memory, input, and connection weights. LSTM can control how long it holds the current memory. A picture illustrating how short term memory is manipulated in LSTM.",
            "slide 19": "A Special Kind of RNN. This kind of RNN can encode a sequence of inputs to an activation pattern in a neural layer and to another sequence from the activity pattern. E.g. We can use this to translate an English sentence into a Spanish one. A picture showing that this kind of RNN can translate an English sentence into a Spanish sentence.",
            "slide 20": "A Special Kind of RNN. The pattern of activation after the RNN that reads the sequence can represent the sentence. 'Thought Vector': If the translation requires thinking about the sentence’s meaning, the representation of a sentence can be thought of as some aspect of the 'meaning' of the sentence. A picture of forward RNN and backward RNN.",
            "slide 21": "Outline. How Do Voice-Controlled Personal Assistants Work?. Speech Recognition. Sequence Processing. Task Execution (highlighted).",
            "slide 22": "Task Execution. The task of center unit (between speech recognition and production) is to translate the voice input into commands that assistant can execute. E.g. 'turning on a light', 'showing web search results'. If the command is very complicated, it sends a request through the Internet to a bigger computer system (server) for processing. A picture of a server.",
            "slide 23": "Voice Synthesis. Voice Synthesis is the final stage of a voice-controlled personal assistant. Speech synthesis is accomplished using an RNN that converts a series of linguistic sounds into a series of spectrum. A picture of chat history of a human with a bot.",
            "slide 24": "Summary. We’ve looked at a voice-controlled personal assistant, which is just one example of an AI-powered natural language processing application. A chatbot is another interesting type of application that shows how AI-based natural language processing can be used. Currently, chat bots are used for entertainment, customer support, online shopping, and so on. Two pictures of chatbots.",
            "slide 25": "Q&A. Lecture 9.",
            "slide 26": "A picture of hearing aid."
        },
        "lecture 10": {
            "slide 1": "SEEING ARTIFICIAL NEURAL NETWORKS. Lecture 10.",
            "slide 2": "Outline. Artificial neural networks can see the world (highlighted). What they see. How they understand: CNN.",
            "slide 3": "Health Industry. AI doctors help to locate tumor regions. Magnetic resonance images are processed by computer and helps doctors to classify the status and type of the tumor in brain. A picture of a brain MRI image.",
            "slide 4": "Facial recognition system. Computers/mobile devices are able to tell who you are. Windows Hello from Microsoft or Face ID from Apple are two commercial examples. A picture of a laptop with Windows Hello.",
            "slide 5": "Outline. Artificial neural networks can see the world. What they see (highlighted). How they understand: CNN.",
            "slide 6": "Videos and images. A video comprises several images, which is called frame. A picture of a flip book. A picture illstrating the concept of a frame.",
            "slide 7": "Videos and images. Commonly, 24 frames are shown within a second, which is termed as frame rate and the unit is frames per second (fps). A video showing the difference between different fps. A picture illstrating the concept of a frame.",
            "slide 8": "Images are stored as pixels. When zooming in a picture real close, we find it is composed of mosaics. These small squares are pixels. We fill each pixel with color and then we get a picture! A picture showing the effect of zooming in a picture will get mosaics or pixels.",
            "slide 9": "What exactly is stored in computer? Red, green and blue are three primary colors. When blend together, they can generate a full range of colors. The computer actually stores the colors of each pixel, and in our case, the value of red, green and blue. A picture showing RGB are the primary colors. A picture of a traffic sign and its corresponding pixel values representation.",
            "slide 10": "Outline. Artificial neural networks can see the world. What they see. How they understand: CNN (highlighted).",
            "slide 11": "The jigsaw puzzle. Computers assemble visual images in the same way you might put together a jigsaw puzzle. Computers distinguish many different pieces of the image, they identify the edges and then model the subcomponents. Using filtering and a series of actions through deep network layers, they can piece all the parts of the image together, much like you would with a puzzle. A picture of a jigsaw puzzle.",
            "slide 12": "Neural networks (CNN) learn from labelled images. The CIFAR-10 and CIFAR-100 are labeled subsets of the 80 million tiny images dataset. They were collected by Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. A picture showing labelled images as inputs to a CNN to let the CNN learn what are planes and what are automobiles.",
            "slide 13": "Convolutional neural networks components. Convolution layers. RELU. Pooling layers. A picture of different layers of a classic CNN",
            "slide 14": "Convolution layer. The convolution layer is the core building block of the CNN. This layer performs a dot product (one-to-one) between two matrices (ordered numbers). One is the image, and the other is called kernel, mimicing the way human visual system works. A picture showing the computation process of convolution.",
            "slide 15": "Convolution layer in animation and why we convolutes?Dr. Hubel and Dr. Wiesel found that some neurons fired very rapidly by watching the lines at specific angles, while other neurons responded best to the lines at different angles. The kernels (smaller matrix) are similar to the neurons: some produce larger output when given certain features like horizontal edges, while some respond to vertical edges. An animation of convolution computation.",
            "slide 16": "Kernel size, stride and the output size. Formulas: Original size: W * H. Kernel size: K * K. Stride: S. W_old = 1 + (W-K)/S. H_old = 1 + (H-K)/S. A picture of a step in convolution.",
            "slide 17": "ReLU, a math transformation. Rectified Linear Unit. output = input if input > 0, 0 otherwise. output = max(input, 0). Mimics the way how biological neurons are triggered! A picture of different numbers and their output after ReLU.",
            "slide 18": "Pooling layer. Pooling condense the output of small regions of neurons into a single output. This helps simplify the following layers and reduces the number of parameters that the model needs to learn. Max pooling: take the largest number (maximum) in the region. Average pooling: take the average of the numbers in the region. A picture showing max pooling and average pooling.",
            "slide 19": "Feature maps. Three pictures of three different feature maps",
            "slide 20": "Convolutional neural networks. A CNN can have tens or hundreds of hidden layers that each learn to detect different features in an image. Classification part produces the final prediction of what is in the image using the information gathered from hidden layers. A picture of different layers of a classical CNN.",
            "slide 21": "AlexNet: A CNN model follows previous pattern. AlexNet won the 2012 ImageNet competition, where computers should tell what is in the picture, with a top-5 error rate of 15.3%, compared to the second place top-5 error rate of 26.2%. top-5 error: predicting 5 categories and if all are wrong, it counts as an error. A picture of AlexNet architecture. A bar graph of classification error rate vs different method groups",
            "slide 22": "THANK YOU! Q&A. Lecture 10."
        },
        "lecture 11": {
            "slide 1": "Artificial and Biological Neural Networks.Lecture 11.",
            "slide 2": "Outline. Why should we study the brain? (highlighted). Areas where ANN can learn from humans. Areas where ANN can learn from the brain. Brain-machine interface.",
            "slide 3": "Introduction. ANNs were originally inspired by the human brain. Many functions of an ANN were modeled after biological neurons. Biological neurons at an abstract level: Multiplie the inputs with the connection weights, Accumulate the results, Apply output function. A picture of a neuron.",
            "slide 4": "The interaction has decreased. A real neuron is made of tiny electrochemical processes that are only roughly approximated in an artificial neuron model. It is unlikely that backpropagation is going on in the brain when people learn, while backpropagation is a powerful algorithm for training ANNs. A picture of backpropagation.",
            "slide 5": "We should watch for developments in neuroscience. There are still areas where people perform much better than AI. There may be a different operating principle we can learn from biological neural networks. Two pictures that show the similarty between a biological neuron and the one in ANN.",
            "slide 6": "Outline. Why should we study the brain?. Areas where ANN can learn from humans (highlighted). Areas where ANN can learn from the brain. Brain-machine interface.",
            "slide 7": "Areas where ANN can Learn from Humans. The human brain works better than ANN in some ways. For example, humans, even children, learn faster than ANN. People require fewer examples to learn a concept and people also tend to make less mistakes than do ANN. A picture of a word cloud about learning.",
            "slide 8": "Graphical model. Graphical model is a new kind of fast-learning algorithm. Nodes: describe anything that can be represented by numbers or labels. Arrows and lines: show one node’s value ends up with the given value of another node. A picture of an example graph.",
            "slide 9": "Graphical model. Five variables: difficulty, intelligence, grade, SAT, letter. Relationship: The difficulty of the course and the intelligence of the student affect the student’s grade. The SAT score is only influenced by the student’s intelligence. The grade affects whether the student gets a good recommendation letter. A picture of a graph representing this relationship.",
            "slide 10": "Few-shot learning. Few-shot learning aims to build accurate machine learning models with less training data. A picture highlighting the difference between large data training and few-shot training.",
            "slide 11": "Few-shot learning. 3 training images of tigers. A test image of a lion and 3 classes: deer, tiger, and wolf. Which class does the test image belong to? Four pairs of images showing different and same categories of objects.",
            "slide 12": "Generative model. Another thing that humans have but machines seem to lack is creativity. ANN models called the generative model can create examples after learning. Generative adversarial network (GAN) learns to generate outputs indistinguishable from the real data. Images generated by GAN.",
            "slide 13": "GAN in details. A picture of the GAN architecture (random noise to generator to generate a fake image and a discriminator trying to discriminate that from the training image)",
            "slide 14": "Transfer learning to new areas. Humans also seem to have the ability to transfer learning to new areas. A network trained for one task may be able to be trained for another task faster if the original task and the applied task have some common features. A picture showing transfer learning where the source data/labels are larger than the target data/labels.",
            "slide 15": "Transfer learning in CNNs. ConvNet as pre-trained model for new tasks! A picture of the architecture of a classical CNN.",
            "slide 16": "Outline. Why should we study the brain?. Areas where ANN can learn from humans. Areas where ANN can learn from the brain (highlighted). Brain-machine interface.",
            "slide 17": "Computational neuroscience. Computational neuroscience examines how the small components of neurons and synapses work, and then analyzes in detail the raw signals the neurons create. Computational neuroscientists record from a single cell or put sensors in the brains of mice. Researchers also run computer simulations of minute process within a neuron. A picture of a computer simulation of the activities of a small circuit in the rat brain.",
            "slide 18": "Neurons work at the cell level. Neurons aren’t just the simple calculators that multiply and add, smaller processes involving molecules carry electric charges. A single cell may perform a very complex operation that isn’t considered in simple neuron models: The positions of the synapses, the merging order of the dendrites. A picture of a neuron structure.",
            "slide 19": "Complex processing of cells. There are fast-changing connections that occur many times per second. These quick connection weight changes can change the grouping of the cells that work together, which makes complex temporal processing possible. A picture of a neuron structure.",
            "slide 20": "The structure of human brain. The “big brain” of humans is made of six layers organized into columns. We don’t precisely understand how the layers and columns work although we know how these cell types are distributed in the layers. Once we find out, the understanding of brain may change and impact ANN technology. A picture showing the six layers of a human brain.",
            "slide 21": "Human brainwaves. Humans and many animals have rhythmic electrical activities in the brain called brainwaves. Big brainwaves can be recorded using an EEG, which involves putting electrodes on the skin of the head and recording voltages. A picture of an EEG.",
            "slide 22": "Human brainwaves. Brainwaves are an important part of information processing in the brain. E.g. we may predict what the mouse will do by recording the brainwaves. Most ANN models don’t generate brainwaves, which means that the real brain has some different operating principles. A picture of a brainwave.",
            "slide 23": "Summary. It may take a long time before the research of brain works will be relevant to ANNs. It’s up to future AI researchers and developers to decide whether ANNs can incorporate some of the findings and ideas from neuroscience. A picture of a brain.",
            "slide 24": "Outline. Why should we study the brain?. Areas where ANN can learn from humans. Areas where ANN can learn from the brain. Brain-machine interface (highlighted).",
            "slide 25": "Brain-machine interface. We can determine what a person has decided in his or her mind by reading brain images. From neurons to machines: We can recognize the decision from measured activities and how to control machines use that knowledge. A picture of a man controlling a robot car with his brainwaves.",
            "slide 26": "Brain-machine interface in the opposite direction. From machines to neurons: People who can’t hear can sense sounds by connecting a sound analyzer. People who are blind can sense brightness and shapes by connecting light sensors to sensory neurons. A picture of a man controlling a robot car with his brainwaves.",
            "slide 27": "Summary. Why should we study the brain?. Areas where ANN can learn from humans. Areas where ANN can learn from the brain. Brain-machine interface.",
            "slide 28": "Thanks! Q&A. Lecture 11.",
            "slide 29": "Graphical model: recall probability theory. Random experiment: Observe something uncertain. Outcome: The result of random experiment. Random variable: A mapping from possible outcomes to real numbers. Possible outcome: Tail or Head. Random variable: use capital letters A/B/C/...Tail -> 0. Head -> 1. P(X=0)=P(X=1)=0.5. A picture of a coin.",
            "slide 30": "Graphical model. Five variables: D(ifficulty), I(ntelligence), G(rade), SAT, L(etter). A table showing P(I=1)=P(I=0)=0.5. Another table showing the probability of SAT=0/1 when I is given.",
            "slide 31": "A table showing probabilities."
        },
        "lecture 12": {
            "slide 1": "ARTIFICIAL INTELLIGENCE: TODAY AND TOMORROW. LECTURE 12",
            "slide 2": "Outline. Cutting edge (highlighted). Future. Proceed with care.",
            "slide 3": "Usage of AI. Image of a voice-controlled personal assistant. Image of a self-driving car. Image of a brain MRI.",
            "slide 4": "Medicine. finding new drugs. medical diagnoses. With the vast knowledge of paintents , and chemical sturctures sotred in databases, AI system can search for potentially effective treatments more quickly than human experts can. A video about AI usage in medicine.",
            "slide 5": "Natural language application. machine translation. natural language interfaces. AI systems extract knowledge from natural language texts. E.g., Q: The headquarters of BHP Billiton Limited, and the global headquarters of the combined BHP Billiton Group, are located in Melbourne, Australia. A: headquarters('“'BHP Biliton Limited', 'Melbourne, Australia') A cartoon picture showing that.",
            "slide 6": "Taking care of us. Robots that take care of patients in general are in development. A picture of an old lady holding the hand of a robot. Robot as a friend. Robot “reads” your emotions and then try to make you feel better. A picture of a robot. ",
            "slide 7": "Usage in dangerous situations. Situations human want to avoid: Old minefields, automobile factories, other planets... AI can make machines in these situations more autonomous and useful. A picture of Curious, a robot that was sent to Mars.",
            "slide 8": "Finance. Stock prediction: This problem has always been a battle ground for different methods. AI can combine mathematical analysis, news analysis, and emotional analysis of the market to provide an accurate prediction of stock prices. A picture of stock price vs time named 'The God Stock Index Prediction'.",
            "slide 9": "Summary. The real-world aplication using AI have just begun, and AI technology hasn’t fully matured and been tested. AI-related news should be read with caution. A picture of a robot.",
            "slide 10": "Outline. Cutting edge. Future (highlighted). Proceed with care.",
            "slide 11": "Integrate with our brains. We expect our brains will be able to communicate with AI through implanted electrodes to expand out cognitive capabilities. Development in this direction may take many years because we must do more research on the human brain side. A picture of a human brain but it's digital.",
            "slide 12": "Solve some of the gravest human problems. poverty, hunger, climate change. Global problems require fast global data acquisition, statistical analysis, knowledge, and strategic thinking, which AI will continue to improve on. A picture of giant machines.",
            "slide 13": "Supporting hardware. Spiking neural networks: They consider the spike timings rather than just the spiking frequency, which is an important feature of biological neural networls. An animation demonstrating spiking neural networks.",
            "slide 14": "Quantum computer. Quantum computers will be faster and smaller. Traditional: Bits. Quantum: Qubits. Represent numbers at the same time. In situations where there are a large number of possible combinations, quantum computers can consider them simultaneously. A picture of a futuristic chip.",
            "slide 15": "Artificial general intelligence (AGI). AGI is a generic, human-like intelligence that can tackle problems by developing and applying general congnitive abilities across different areas. Many people think that the progress in AI will be made only by tackling specific problems, not by trying to advance machine intelligence in general. A picture of a robot trying to solve complex math/physics problems.",
            "slide 16": "Artificial general intelligence (AGI). A machine use millions of cameras or microphones that it isn't limited by human vision or hearing. Machines can integrate all the domain-specific knowledge and add a module that has human-like intuitions as a legacy component. The world’s top-level researchers are working to develop AGI. A picture of a brain floating on a chip.",
            "slide 17": "Outline. Cutting edge. Future. Proceed with care (highlighted).",
            "slide 18": "Safety issues. The ANN components of AI can drive a car very well but can’t explain how it’s doing the job. If something goes wrong, who is to blame? The developer of the automobiler? The develooper of the AI module? A picture of a futuristic car.",
            "slide 19": "Ethical issues. If AI are developed with more sensitive sensors and more autonomous control, will robots, at some point, start to have personalities and consciousness? Should we worry about respecting their opinions or their will for existence? A picture of 2 robots.",
            "slide 20": "AI may spell the end to human intelligence. Some of the world’s most intelligent people have warned that AI may spell the end to human intelligence. When AI is more intelligent than we are, we can lose control of it, and we can no longer guatantee that AI will take actions we think of as good. A picture of Stephen Hawking, Elon Musk, and Bill Gates.",
            "slide 21": "Unemployment problems. We already have unemployment problems in many parts of the world. There is no doubt that AI is already replacing some jobs. Sooner or later, AI will surpass human beings in terms of intelligence. Generative ANNs can be trained to create samples of art that it hasn’t seen before. A picture of a robot as a coworker of 2 humans.",
            "slide 22": "Summary. When AI suipass human intelligence, our worth may not be measured by how smart we are, but how wise we are. We should be samrt enough to use AI and wise enough to plan our future so that using AI will enrich our lives. Maybe we should stop developing AI, or maybe we should rest our future in the hands of AI. What do you think? A picture of a child making a robot.",
            "slide 23": "Q&A. Thanks! Lecture 12"
        }
    },
    "elementary school": {
        "lecture 1": {
            "slide 1": "WHAT IS ARTIFICIAL INTELLIGENCE. LECTURE 1",
            "slide 2": "Outline. Artificial intelligence everywhere. Logic. Statistics. Artificial neural networks.",
            "slide 3": "Artificial intelligence everywhere. chatbot on the cell phone. computers help doctors make diagnoses. some robots can understand emotions. some cars can drive without drivers. It contains four pictures that illustrate four points discussed in this slide.",
            "slide 4": "Birth of AI. 1956 Workshop at Dartmouth College. John McCarthy. Marvin Lee Minsky. Claude Elwood Shannon. -Name 'Artificial Intelligence' for the new field. It contains three pictures of the three people mentioned in this slide.",
            "slide 5": "Artificial Intelligence. According to Wikipedia AI is 'intelligence demonstrated by machines in contrast to the natural intelligence displayed by humans and other animals'. It contains a picture of a robot.",
            "slide 6": "Outline. Artificial intelligence everywhere. Logic. Statistics. Artificial neural networks.",
            "slide 7": "Logic. Logic: a method of using facts and rules to decide on the truth of a statement. e.g. 'Candies are sweet' and 'A lollipop is a candy' we can infer 'A lollipop is sweet'. It contains a picture of an example of logic.",
            "slide 8": "Logic used in AI. Deduction infer new facts from already known facts and rules. Induction learn by generating rules from known facts. store the knowledge. computer can think. Logic.",
            "slide 9": "Problems with logic-based AI. A lot of common sense knowledge is required and it’s almost impossible to gather a sufficient amount of this knowledge. The computer must know the real-world meaning of words. Simple symbolic logic does not work well when things are uncertain. It contains two pictures of the problems with logic-based AI. One is chocolate and the other is about the weather.",
            "slide 10": "Outline. Artificial intelligence everywhere. Logic. Statistics. Artificial neural networks.",
            "slide 11": "Statistics. Statistics talks about the probability of something happening deals with uncertainties. The probability of an event is a number between 0 and 1. It contains a picture demonstrating the probability.",
            "slide 12": "Example of statistics. 'people have more headaches on rainy days'. - Bayes’ rule named after Thomas Bayes. It contains a picture of Bayes and an example of Bayes’ rule.",
            "slide 13": "Statistics in AI. Strength (rigorous mathematical formulation). Once we reach a conclusion it’s possible to explain how you reached that conclusion and your degree of certainty about whether your conclusion is correct. Limitations (complex situations). Finding the best underlying structure is difficult in complex situations.",
            "slide 14": "Outline. Artificial intelligence everywhere. Logic. Statistics. Artificial neural networks.",
            "slide 15": "Brain. A human brain is made of 100 billion tiny cells called neurons. (1) inputs that collect signals from other neurons. (2) a decision-making section. (3) output that sends out its decision. A neuron learns by changing the connection weight to a synaptic weight. It contains two pictures of neurons.",
            "slide 16": "Artificial Neural Network. ANNs are some of the most important components of the AI that allows machine to learn from experience that is inspired by the way neurons work. Input processing output. It contains two picutres of neuron and neural network.",
            "slide 17": "Artificial Neural Network. Multiple artificial neurons connected to each other and become an artificial neural network. Improve the effectiveness of the ANN by changing the weight of the connection. It contains two picutres of neuron and neural network.",
            "slide 18": "Three ways of learning. Supervised learning neural networks are given training data and then compute an output which they compare to the desired output provided by the teacher. effective but requires that all input data are paired with correct answers. It contains a picture of supervised learning.",
            "slide 19": "Three ways of learning. Unsupervised learning is without a teacher the ANN is given a lot of examples and tries to figure out how data can be organized. Don’t need correct answers but not possible for everything. It contains a picture of unsupervised learning.",
            "slide 20": "Three ways of learning. Reinforcement learning is learning from a teacher who lets you try and then tells you how well you did. Takes a long time sometimes the only way. It contains a picture of reinforcement learning.",
            "slide 21": "Outline. Artificial intelligence everywhere. Logic. Statistics. Artificial neural networks.",
            "slide 22": "THANKS. Q&A. LECTURE 1"
        },
        "lecture 2": {
            "slide 1": "THE DEVELOPMENT OF ARTIFICIAL INTELLIGENCE. LECTURE 2",
            "slide 2": "Outline. The beginning. The first neural networks winter. The second neural networks boom. The second neural networks winter. The third neural networks boom.",
            "slide 3": "Structure of brain. A human brain is made of 100 billion tiny cells called neurons. Brain is a network of neurons. It contains two pictures of brain and neurons.",
            "slide 4": "Knows the neural network of brain. The reticular theory: the nervous system was a continuous network. The nervous system transmits signals between different parts of the body. Camillo Golgi invented a groundbreaking method for observing the nervous system. Camillo Golgi (1843-1926). It contains two pictures of Camillo and neurons.",
            "slide 5": "Knows the neural network of brain. The neuron doctrine: neurons were discrete building blocks. Santiago Ramon y Cajal made observations of the microstructure of the nervous system. Santiago Ramon y Cajal (1852-1934). Ramon y Cajal’s drawing of the cells. It contains two pictures of Santiago and neurons.",
            "slide 6": "Logic gates. Logic gates are physical electronic devices every kind of logic gate has the corresponding input and output rules. AND. OR. NOT. It contains three pictures of logic gates.",
            "slide 7": "neuron could act as a logic gate. McCulloch and Pitts thought each neuron could act as a logic gate and that it could be combined to form a complex computing system. McCulloch and Pitts. A neuron can receive inputs from other neurons, weighs and sums them up, and decides whether or not to produce an output. It contains three pictures of McCulloch and neurons.",
            "slide 8": "Ability to learn. Intelligent machines must be able to learn just like the human brain. Donald Hebb the first learning theory The Organization of Behavior (1949). When two neurons get excited together the connection weights increase. Frank Rosenblatt the first ANN model with learning capability named perceptron. It was designed and realized as an image recognition machine.",
            "slide 9": "Outline. The beginning. The first neural networks winter. The second neural networks boom. The second neural networks winter. The third neural networks boom.",
            "slide 10": "The first neural networks winter. Rosenblatt could not come up with an effective algorithm for adjusting the weights in the hidden layer neurons. It was easy to adjust the weights to the output there were no effective ways to adjust the weights in hidden layers. It contains a picture of the neural network.",
            "slide 11": "Outline. The beginning. The first neural networks winter. The second neural networks boom. The second neural networks winter. The third neural networks boom.",
            "slide 12": "The second neural networks boom. Several people found ways to train multiple layer neural networks through a technique called backpropagation. ANNs with multiple layers could be trained to yield outputs closer to the desired output values. Backpropagation provides an effective way to change hidden connections to reduce output errors. It contains a picture of the backpropagation.",
            "slide 13": "Outline. The beginning. The first neural networks winter. The second neural networks boom. The second neural networks winter. The third neural networks boom.",
            "slide 14": "The second neural networks winter. Training multilayer ANNs takes a lot of computation. ANN applications were limited to small data and the results of ANNs could not compete with traditional algorithms. People became less interested in neural network research 1990s. It contains a picture of the AI winter.",
            "slide 15": "Outline. The beginning. The first neural networks winter. The second neural networks boom. The second neural networks winter. The third neural networks boom.",
            "slide 16": "Computing speed accelerated. Graphics processing units (GPUs) accelerated the speed of the computing that ANNs required. Moore’s law states that the number of transistors on computer chips doubles every two years. It contains a picture of the GPU.",
            "slide 17": "Current state of ANNs. Interpretability. It’s difficult to interpret what ANNs are doing raises safety concerns. Interpretability: AI can recognize pictures of dogs but we don’t know how it operates. It contains a picture of a dog and the neural network.",
            "slide 18": "Current state of ANNs. Generalizability. ANNs don’t generalize when they encounter inputs different from those they are trained on. Generalizability: When the AI encounters a new batch of pictures of dogs its performance will be different. It contains a picture of a dog and the neural network.",
            "slide 19": "Outline. The beginning. The first neural networks winter. The second neural networks boom. The second neural networks winter. The third neural networks boom.",
            "slide 20": "THANKS. Q&A. LECTURE 2"
        },
        "lecture 3": {
            "slide 1": "ARTIFICIAL NEURAL NETWORKS THAT LEARN FROM TEACHERS. LECTURE 3",
            "slide 2": "Outline. Supervised learning: learning from a teacher. Computing the output. Comparing the output to the desired output. Changing the connection weight. Backpropagation.",
            "slide 3": "Supervised learning. A supervised learning ANN learns from a teacher. After the network is given the correct answer it can then adjust its connection weights to arrive at correct answers. It contains two pictures of learning and neural network.",
            "slide 4": "A simple network (more details). Has two artificial neurons (green stars in the figure) specialized in finding the letter x and the symbol +. One-layer network. It contains a picture of the neural network.",
            "slide 5": "The delta rule. Delta: symbolizes a change or difference. Uppercase: Δ and lowercase: 𝛿. The delta rule for linear artificial neuron is. It contains a picture of formula.",
            "slide 6": "Outline. Supervised learning: learning from a teacher. Computing the output. Comparing the output to the desired output. Changing the connection weight. Backpropagation.",
            "slide 7": "Computing the output. Our inputs are: 'x' and '+'. Assume that the weights are all 0.01. Then the weight pattern of the artificial neurons would look like. It contains three pictures of the input, weight, and output.",
            "slide 8": "Computing the output. Output for x = sum of (input * weight) for each input plate ...",
            "slide 9": "Outline. Supervised learning: learning from a teacher. Computing the output. Comparing the output to the desired output. Changing the connection weight. Backpropagation.",
            "slide 10": "Comparing the output to the desired output. Output: 0.05 Target: 1. It contains a picture of an example.",
            "slide 11": "Outline. Supervised learning: learning from a teacher. Computing the output. Comparing the output to the desired output. Changing the connection weight. Backpropagation.",
            "slide 12": "Changing the connection weight. The delta rule states that the change of weight depends on both the input and the desired change. Learning constant: if it is smaller than 1 adjust the weight little by little. It contains a table describing the input and weight change.",
            "slide 13": "Example: Weight change for the 'x' neuron. It contains an example of weight change.",
            "slide 14": "Example 2: Weight change for the 'x' neuron. For input plates whose values are 0. It contains an example of weight change when inputs are 0.",
            "slide 15": "Example 3: Weight change for the 'x' neuron. After applying the delta rule the weight pattern of the 'x' artificial neuron will be. It contains a picture of calculation.",
            "slide 16": "Summary. ANNs work by training the network for samples 'x' and '+' repeatedly. The output of the artificial neuron will become closer and closer to the desired output. It contains a table of relationship between input and neuron.",
            "slide 17": "Outline. Supervised learning: learning from a teacher. Computing the output. Comparing the output to the desired output. Changing the connection weight. Backpropagation.",
            "slide 18": "Backpropagation. Backpropagation was developed to train incoming connection weights to hidden layers. It provides a method to train multilayer neural networks.",
            "slide 19": "Backpropagation. Extends the delta rule in two steps: change the way neurons compute their outputs. add a rule for error backpropagation. It contains a picture of the backpropagation.",
            "slide 20": "Change in the Computation. The delta rule: a neuron computes 'the weighted sum of inputs' is extended as: a neuron computes some math transformation of the weighted sum. Mimics the way how biological neurons are triggered!",
            "slide 21": "Change in the Computation. The output is 5 (because of ReLU) the desired output change: 95 error: the deviation of the actual output from the desired output",
            "slide 22": "Change in the Computation. Contribution ratio: the ratio between the inner state change and the output change. The weight change proportional to: input * contribution ratio * error = 1*1*95",
            "slide 23": "Change in the Computation. Learning rate: 0.01. It contains a picture of the calculation.",
            "slide 24": "Error back propagation. Backpropagation: the propagation of error from the output layer to the middle layer before it. (Backwards!) Lower layers accumulate error from high layers.",
            "slide 25": "Summary. Supervised learning: learning from a teacher. Computing the output. Comparing the output to the desired output. Changing the connection weight. Backpropagation.",
            "slide 26": "THANKS. Q&A. LECTURE 3"
        },
        "lecture 4": {
            "slide 1": "Algorithms that learn from teachers. Lecture 4",
            "slide 2": "Outline. What is a decision tree. How to build a decision tree. K-Nearest Neighbors (KNN). The Effect of K",
            "slide 3": "How do humans classify. Humans use objects’ characteristics to classify objects resulting in the label of objects.",
            "slide 4": "What is a decision tree. A decision tree is a flowchart-like diagram that shows the various outcomes from a series of decisions. It can be used as a decision-making tool for research analysis or for planning strategy. It contains a picture of decision tree.",
            "slide 5": "The structure of a decision tree. Internal node. Branch. Leaf node. It contains a picture of the structure of a decision tree.",
            "slide 6": "More details of a decision tree. In the prediction a certain attribute is used to make a judgment at the internal nodes of the tree. According to the judgment result which branch to enter is decided. When it reaches the leaf node the classification result is obtained. It contains a picture of the decision tree.",
            "slide 7": "An example of classifying fruits. We find a most useful feature such as shape: curve or round. Split the dataset: If the fruit is curved then it is a banana. Select the next best feature until each node contains only one class. At last we build a decision tree. The picture contains an example of decision tree.",
            "slide 8": "Outline. What is a decision tree. How to build a decision tree. K-Nearest Neighbors (KNN). The Effect of K",
            "slide 9": "How to build a decision tree? When we face many features it is hard for us to build tree by hand. Can we teach machine to find it? How to find the most useful feature on each node? When should we stop growing the tree? It contains an example of decision tree table.",
            "slide 10": "Feature selection. Which one is better? The left. Feature selection is to screen out the features with high correlation with the classification results or the features with strong classification ability. It contains a picture of feature selection.",
            "slide 11": "Outline. What is a decision tree. How to build a decision tree. K-Nearest Neighbors (KNN). The Effect of K",
            "slide 12": "Machine Learning: Geometric View. We can view examples as points in a d-dimensional space where d is the number of features. Assumption: Closer points in feature space have similar semantics. It contains a picture of feature space",
            "slide 13": "I-Nearest-Neighbor (1NN). To classify a new example x: Label x with the label of the closest example to x in the training set. Closest to red class. Euclidean Distance. It contains a picture of 1NN.",
            "slide 14": "K-Nearest-Neighbors (KNN). x is closest to a red point. But most of the next closest points are blue. If we only consider the closest point the result won’t be accurate enough. It contains a picture of KNN.",
            "slide 15": "K-Nearest-Neighbors (KNN). K-Nearest-Neighbor (KNN): Find k nearest neighbors of x. Label x with the majority label within the k nearest neighbors. Consider: How can we handle ties for even values of k? It contains a picture of KNN.",
            "slide 16": "Outline. What is a decision tree. How to build a decision tree. K-Nearest Neighbors (KNN). The Effect of K.",
            "slide 17": "The effect of K. Increasing k simplifies the decision boundary. Majority voting means less emphasis on individual points. Fit Noise. Smooth Boundary. I-Nearest-Neighbor. I5-Nearest-Neighbor. It contains a picture of the effect of K.",
            "slide 18": "The Effect of K. Choose best k on the validation set. (Often set k as an odd number). It contains a graph of choosing the best k.",
            "slide 19": "KNN Summary. Advantages: Data number can be very large. Class number can be very large. All other ML algorithms may fail here! Disadvantages: Slow at inference time. Fooled easily by irrelevant attributes",
            "slide 20": "Thanks. Q&A. Lecture 4"
        },
        "lecture 5": {
            "slide 1": "ARTIFICIAL NEURAL NETWORKS THAT LEARN FROM EXAMPLES (I). Lecture 5",
            "slide 2": "Outline. Unsupervised learning: Learning without a teacher. Unsupervised learning: How to choose features. Unsupervised ANNs that make maps. How a self-organizing features map learns.",
            "slide 3": "Unsupervised learning. Supervised Learning: Requires a teacher to tell the ANN the correct answer. Unsupervised Learning: Learns by examples without the correct answer. E.g. ANNs are given images of apple and images of orange without being told what the fruit is. ANN learns to group them into apples and oranges. It contains a picture of an example of unsupervised learning.",
            "slide 4": "Other Application Scenarios. Unusual bank users found. Some people use bank accounts to commit illegal acts. It is difficult to find out these bank users through human analysis. Unsupervised learning helps us to quickly classify behaviors. We can quickly exclude normal users and conduct in-depth analysis of abnormal behaviors in a more targeted manner.",
            "slide 5": "Other Application Scenarios. User segmentation for advertising. The more targeted advertising can be the better the effect will be. Through unsupervised learning we can not only segment users according to dimensions such as gender, age, and geographic location, but also based on user behavior.",
            "slide 6": "Outline. Unsupervised learning: Learning without a teacher. Unsupervised learning: How to choose features. Unsupervised ANNs that make maps. How a self-organizing features map learns.",
            "slide 7": "Features. Why this grouping is feasible? ---apples (oranges) are similar to each other. Features: Characteristics of the objects selected to make all the inputs widely different from each other. ANN has to pay attention to useful features! Useful (important): colors, textures. Unreliable: round, shiny.",
            "slide 8": "Features. What should good features look like? The samples have to be quite different in this feature. E.g. color (useful) round (unreliable). The feature that is highly correlated with the target sample. E.g. color (useful) ear shape (unreliable).",
            "slide 9": "Outline. Unsupervised learning: Learning without a teacher. Unsupervised learning: How to choose features. Unsupervised ANNs that make maps. How a self-organizing features map learns.",
            "slide 10": "Self-Organizing Feature Map (SOFM). SOFM: an unsupervised learning ANN model. Invented by Teuvo Kohonen, a Finnish researcher in engineering. A tool for mapping complex information on a sheet. It contains a picture of Teuvo.",
            "slide 11": "Self-Organizing Feature Map (SOFM). SOFM learns to create a neural map: Similar inputs are placed in nearby locations. Inputs presented more frequently occupy more areas. The more a specific type of input is seen, the more minute differences among the inputs belonging to that type are represented in the map. It contains a picture of SOFM.",
            "slide 12": "Example of Self-Organized Structure. Primary sensory area: Two-dimensional map of the body preserves the topology of the body similar to the map of an SOFM. The relative area size of each part of the body is determined by how much that part is used.",
            "slide 13": "Example of Self-Organized Structure. Primary Visual Cortex: (also in the brain). David Hubel and Torsten Wiesel: primary visual cortex containing a subgroup of neurons sensitive to a particular orientation. Experiment: If one of a young cat’s eyes is kept shut, the formation of the primary visual cortex is disrupted. The development of the primary visual cortex involves a self-organizing process. It contains two pictures of the primary visual cortex.",
            "slide 14": "Outline. Unsupervised learning: Learning without a teacher. Unsupervised learning: How to choose features. Unsupervised ANNs that make maps. How a self-organizing features map learns.",
            "slide 15": "How an SOFM Learns. SOFM learns by being exposed to input samples. Similar inputs activate nearby neurons, the neighborhood shrinks as the network learns more samples. The inputs should be the same size. It contains a picture of SOFM.",
            "slide 16": "How an SOFM Learns. Step 1: Initially set map to a random map. Step 2: Provide an input to the SOFM. In our case, a picture of orange/apple. All neurons will respond to the new input. It contains a picture of SOFM.",
            "slide 17": "How an SOFM Learns. Step 3: We select the one with the strongest response. We consider neighbor neurons close to that neuron within the radius. Step 4: Modify all the neurons in the neighborhood so they all respond largely to the input. It contains a picture of SOFM.",
            "slide 18": "How an SOFM Learns. Step 5: Slightly reduce the neighborhood size (radius) and the rate we change the neighbor neurons. Step 6: Go back to step 2. Repeat. It contains two pictures of SOFM.",
            "slide 19": "Summary. Unsupervised learning: Learning without a teacher. Unsupervised learning: How to choose features. Unsupervised ANNs that make maps. How a self-organizing features map learns.",
            "slide 20": "THANK YOU! Q&A. Lecture 5"
        },
        "lecture 6": {
            "slide 1": "ARTIFICIAL NEURAL NETWORKS THAT LEARN FROM EXAMPLES (II). Lecture 6",
            "slide 2": "Outline. Unsupervised ANNs that make groups. Adaptive resonance theory 1. K-means Clustering. Unsupervised ANNs that compress.",
            "slide 3": "Grouping. An unsupervised learning neural network can group similar inputs. Grouping is also called clustering classification or categorization. E.g., grouping images of cats and images of dogs. The picture contains an example of grouping.",
            "slide 4": "Adaptive resonance theory 1 (ART1). Adaptive resonance theory 1: an example of a neural network that performs grouping. Created by cognitive scientist Stephen Grossberg and mathematician Gail Carpenter. It contains a picture of Stephen.",
            "slide 5": "Adaptive resonance theory 1 (ART1). ART1 can form stable memories by creating 'resonance' between two flows of data. Bottom-up activation from the input. Top-down activation from the memory. To make this happen an ART network has three layers: input (bottom) interface (middle) category (top). It contains a picture of the structure of ART1.",
            "slide 6": "Details about ART1. Input layer: sends signals to the interface layer through one-to-one connections. Interface layer: activates the category layer through full connections. Category layer: sends back activations to the interface layer through full connections. The interaction between the interface and category layer weights creates resonance. It contains a picture of the structure of ART1.",
            "slide 7": "Outline. Unsupervised ANNs that make groups. Adaptive resonance theory 1. K-means Clustering. Unsupervised ANNs that compress.",
            "slide 8": "K-means Clustering. K-means is the most commonly used clustering method. The input is a sample set (point set). The samples with similar characteristics will be clustered into one category. Most of the time the examples will be abstracted into points on the plane. It contains a picture of an example of k-means.",
            "slide 9": "The process of k-means clustering. Step 1: select the number k of categories to be clustered and select k center points. E.g., k=3. The k center points are randomly selected on the plane. It contains a picture of an example of step 1.",
            "slide 10": "The process of k-means clustering. Step 2: For each sample point find the center point closest to it and the points closest to the same center point is a class. Thus completing a clustering. Closest to green! Closest to red! Closest to blue! It contains a picture of an example of step 2.",
            "slide 11": "The process of k-means clustering. Step 3: determine whether the categories of the samples points before and after the clustering are the same. If they are the same, the algorithm terminates. Otherwise, enter step 4. In this example the sample points before and after are different then we enter step 4. It contains a picture of an example of step 3.",
            "slide 12": "The process of k-means clustering. Step 4: for the sample points in each category calculate the center point of these sample points as the new center points. Continue to step 2. It contains a picture of an example of step 4.",
            "slide 13": "The process of k-means clustering. The remaining process of the example: step 2 -> step 3 -> step 4 -> step 2 -> step 3 -> end. It contains a picture of an example of K-means.",
            "slide 14": "Key steps of k-means. Find the center point closest to a sample point. Update the center point.",
            "slide 15": "Outline. Unsupervised ANNs that make groups. Adaptive resonance theory 1. K-means Clustering. Unsupervised ANNs that compress.",
            "slide 16": "Unsupervised ANNs that compress. These networks usually have a middle layer that has a much smaller number of neurons making an hourglass-structure. These networks usually compress the inputs and reproduce the inputs. It contains a picture of the structure of unsupervised ANNs and a picture of biscuits.",
            "slide 17": "Unsupervised ANNs that compress. This network can transform the input to the middle layer code. The network encodes a large input to a small 'code' that still represents the input. Usage: remove noises. If we use an input slightly different from the learned one, an autoencoder tends to create the input that has already been learned. It contains a picture of the structure of unsupervised ANNs.",
            "slide 18": "Image compression with k-means. In the problem of image compression K-means clustering will group similar colors together into k clusters of different colors. The larger the k is, the closer the compressed image is to the original image. It contains a picture of an example of image compression.",
            "slide 19": "Summary. Unsupervised ANNs that make groups. Adaptive resonance theory 1. K-means Clustering. Unsupervised ANNs that compress.",
            "slide 20": "THANK YOU! Q&A. Lecture 6"
        },
        "lecture 7": {
            "slide 1": "ALGORITHMS THAT LEARN THROUGH TRIAL AND ERROR (I). Lecture 7",
            "slide 2": "Outline. Reinforcement learning: the third way (highlighted). Learning the situation(V-learning)",
            "slide 3": "Reinforcement learning: the third way. Supervised learning: Learn from teachers. Unsupervised learning: Learn from examples. Pictures that are related to learning algorithms.",
            "slide 4": "The limitation of supervised / unsupervised learning. Context: A baby is learning to walk. Supervised learning. Tell the baby which muscles should be used? A picture of big red X besides supervised learning. Unsupervised learning. Observe sensory inputs or feedback from the feet and legs alone? A picture of big red X besides unsupervised learning. Randomly generate movements and remember how they work in each situation. A picture of big green check mark besides the last sentence. A picture of a crawling baby.",
            "slide 5": "Reinforcement learning. Reinforcement: When you try something and then get rewarded, you will increase the chance of repeating the action. Reinforcement learning: Operating in a dynamic environment and learning from collected experiences to act correctly. A picture of an agent doing action to the environment and environment returns rewards and observations to the agent.",
            "slide 6": "History of reinforcement learning. Reinforcement learning was originally a concept used in behavioral psychology. Richard Sutton, a Canadian computer scientist, is one of the pioneers in the development of reinforcement learning in the context of AI. He contributed to a RL method called: Temporal Difference learning. A picture of Richard Sutton.",
            "slide 7": "Key conponents of reinforcement learning. A picture of an agent doing action to the environment and environment returns rewards and observations to the agent where the agent is a deep learning model and the environment is the world. Goal: Get most rewards! A picture of a bag of money besides the last sentence.",
            "slide 8": "An example of reinforcement learning. A mouse tries to get some cheese located at the end of a maze. Agent: learner. The mouse. Environment: The maze. Action: agent produces an output. Turning/running. State: the situation of the world observable by the agent. Location in the maze. Reward: cheese. A picture that illustrate this example of reinforcement learning.",
            "slide 9": "An example of reinforcement learning.  agent performs an action -> state changes  -> action depends on the state -> the agent and the state form a loop. The reward is used to change the strategy for choosing the next action, but it is not usually used to decide the next action directly. A picture that illustrates the 'A mouse tries to get some cheese located at the end of a maze' reinforcement learning example.",
            "slide 10": "Train machines to play games. The machine makes moves and learns how much the situation improves or worsens. Improved situations: more points which means that it will eventually win. A picture of two player Pong game.",
            "slide 11": "AlphaGo. AlphaGo was developed by Google’s DeepMind team. Employed reinforcement learning. Beat Lee Sedol, an 18-time Go world champion in 2016. AI outperforming humans at a task using reinforcement learning. A picture of AlphaGo playing Go with Lee Sedol.",
            "slide 12": "Robot Control. Steps: Chooses an action. Receives the evaluation. Improves the selection of the next actions. Fits the model of reinforcement learning! A picture of a configurable robot with modular legs.",
            "slide 13": "Outline. Reinforcement learning: the third way. Learning the situation (V-learning) (highlighted).",
            "slide 14": "Value learning (V-learning). Value learning places a value on the situation. Learning how good or bad the situation is can be important. Value of a situation: The total reward that an agent can expect to collect from that state and onwards into the future. Two animations showing two different total rewards a agent could get by choosing different actions.",
            "slide 15": "The details of V-learning. The system learns to determine the value of a situation. We say the situation is s. e.g. The possible positions of the mouse. We say the current reward is r. reward means the evaluation of some situation, e.g., the cheese. Goal: for every situation s, find the value V. A picture showing a mouse going through a maze where each cell is a state.",
            "slide 16": "The details of V-learning. A simple situation: The current reward is the only indicator of the value in the current situation. state = s. reward = r. Value of s: V_s = r. A table that has a row of states and another row of value that correspond to each state. A picture showing a mouse going through a maze where each cell is a state.",
            "slide 17": "The details of V-learning. A simple situation: The current reward is the only indicator of the value in the current situation. However, r is not always consistent in real world. new state = s. reward = r. Value of s: V = V_old + α * (r - V_old). Averaging the old value with new reward. A picture showing a mouse going through a maze where each cell is a state.",
            "slide 18": "The value function considering future effect. A complex situation: The total reward that an agent can expect to collect from that state and onwards into the future. In Go, we don’t know the outcome until the agent clearly wins. This holds for many board games. In practice, we also consider the effect after the current action. A picture of Go.",
            "slide 19": "Neural network: a more complicated situation. A few states: (3 * 6 = 18 states in the mouse-maze example). We only need a table that stores the average evaluation for each state. Large members of states: (chess has possibly 7.7*10^45 states/arrangments). We need a neural network.We train a neural network to output r (reward) when the input is s (state). A picture of chess.",
            "slide 20": "An classic example of V-learning. A neural pole balancing demonstrated on a computer. states: position and the speed, infinite. action: movement of the palm. reward: we get rewards when the pole is closer to upright. A picture illustrating the experiment.",
            "slide 21": "Outline. Reinforcement learning: the third way. Learning the situation (V-learning). A picture of an agent doing action to the environment and environment returns rewards and observations to the agent where the agent is a deep learning model and the environment is the world. A table of 3 columns: Type, Value of given state s, Condition. The first row of that table is V-learning, r, The current reward is the only indicator of the value. The second row of that table is V-learning, V_old + α * (r - V_old), r is not consistent. The last row of that table is V-learning, Neural network, # states is out of hand.",
            "slide 22": "Q&A. Lecture 7.",
            "slide 23": "The value function considering future effect. Values: g: the degree of the effect of the future situation. (0<g<1, we assume 0.5). V_next : the value of the best possibe situation next (s’). Note: V_next is looked up from the current table. updated value: V_new = V_old + α *((r + g*V_next) - V_old). Note: we use r + 0.5*V_next instead of r. A picture showing a mouse going through a maze where each cell is a state and it illustrates the formula.",
            "slide 24": "Q-learning. We learn how good it is to take a specific action a in the situation s. e.g. s is the arrangement of pieces on a chessboard, a is a possible move. We can look up the resultant evaluation r in a lookup table. The table can be used to locate the best action by looking through the entries with the current situation s. A table with different actions being the columns, different states being the actions, and each cell being the reward corresponding to that state and action.",
            "slide 25": "The details of Q-learning. We can smooth out different evaluations of the same situation-action pair as in V-learning. action that would result in the best next state = a’. the best next state = s’. Q function of a and s: Q_new = Q_old + 0.1*(r-Q_old). A picture of a real life digital mouse trying to find cheese in a maze. A table with different actions being the columns, different states being the actions, and each cell being the reward corresponding to that state and action which is Q_old + 0.1*(r-Q_old).",
            "slide 26": "Outline. Reinforcement learning: the third way. Learning the situation (V-learning). Learning to act in the situation (Q-learning). Autonomous cars (highlighted).",
            "slide 27": "Autonomous cars. Also called self-driving cars. Methods: 1. They use cameras and other sensors to assess driving situations. 2. The program is trained using recorded scenes and stimulators. A picture of one of Google’s Waymo self-driving cars.",
            "slide 28": "Autonomous Cars. Pros and cons: 1. Autonomous cars can maneuver in tricky situations. 2. Dealing with cars driven by humans is difficult. Issues: 1. Responsible for crashes. 2. Privacy issues, including the potentiality for mass surveillance. A picture of One of Uber’s experimental self-driving cars.",
            "slide 29": "Summary. Reinforcement learning: the third way. Learning the situation (V-learning). Learning to act in the situation (Q-learning). Autonomous cars."
        },
        "lecture 8": {
            "slide 1": "ALGORITHMS THAT LEARN THROUGH TRIAL AND ERROR (II). Lecture 8.",
            "slide 2": "Outline. Learning to act in the situation (Q-learning) (highlighted). Autonomous cars.",
            "slide 3": "Quick review: reinforcement learning and V-learning. A picture showing 3 types of machine learning: unsupervised, supervised, and reinforcement. A picture of an agent doing action to the environment and environment returns rewards and observations to the agent. Reward: the instantaneous benefit of being in a specific state, Value: the total reward that an agent can expect to collect from that state and onwards into the future. V-learning learns the value for different situations, i.e. how good the situation is.",
            "slide 4": "Q-learning. V-learning: how good it is for the agent to be in a given state. Q-learning: how good it is to perform a given action in a given state. Breakdown of the interaction between the agent and the environment: Agent collects the initial state S0 of the robot from the environment. Based on S0 state, take action A0. Get reward R1 from the environment. Transition to the new evironment---state S1. A: action  S: state  R: reward. A picture of a robot crossing a thin bridge.",
            "slide 5": "Q-learning. Q-learning algorithm takes as input a series of [state, action, reward]. The ultimate goal of agent is to maximize the overall reward. {S0, A0, R1, S1, A1, R2, S2, ...}. Agent is in state S0 and takes action A0, which causes it to get the reward R1 and be in state S1;It performs A1, receives the reward R2, enters state S2. A picture showing a mouse going through a maze where each cell is a state.",
            "slide 6": "Q-learning. State: (1,1), (2,1), (3,1)... Action: up, down, left, right. Reward: cheese. Agent is in state (1,1) and takes action right, which causes it to get the reward cheese and be in state (2,1); A picture of a 3x6 maze with a cheese at cell (2,1).",
            "slide 7": "The steps of Q-Learning. Initialize Q table. Choose an action a. Perform action. Measure reward. Update Q. Go back to Choose an action a. At the end of the training: Good Q* table. Q-table is a table that records Q-values. Q-value: the maximum expectation of long-term rewards for different actions taken in each state. Value: the total reward that an agent can expect to collect from that state and onwards into the future.",
            "slide 8": "Q-learning details. A simple situation: The current reward is the only indicator of the value in the current situation. A picture of a cheese at cell (2,1) in a 3x6 maze. A table of with entries corresponding to action/state where the actions are going up/down/left/right and the entries corresponding to state (1,1) and (3,1) are all 0s.",
            "slide 9": "Q-learning details. A simple situation: The current reward is the only indicator of the value in the current situation. A picture of a cheese at cell (2,1) in a 3x6 maze. A table of with entries corresponding to action/state where the actions are going up/down/left/right with the entries corresponding to state (1,1) being -1,+100,N/A,-1 and N/A,-1,N/A,+100 for state (3,1).",
            "slide 10": "Q-learning details. However, r is not always consistent in real world. We can smooth out different evaluations of the same situation-action pair as in V-learning. Action that would result in the best next state = a’. Best next state = s’. Q function of a’ and s’: Q_new = Q_old + α*(r-Q_old). A picture of a real life digital mouse trying to find cheese in a maze. A table with different actions being the columns, different states being the actions, and each cell being the reward corresponding to that state and action which is Q_old + 0.1*(r-Q_old).",
            "slide 11": "A missing point: how to choose action? Exploration: Explore new area in the environment. Exploitation: Collect the most reward you already know about. Two animations showing two different total rewards a agent could get by choosing different actions.",
            "slide 12": "Summary. Advantages: Model free of the environment. Disadvantages: In the case of a large number of states. The Q-table will be large and searching and storage will consume a lot of time and space.",
            "slide 13": "Outline. Learning to act in the situation (Q-learning). Autonomous cars (highlighted).",
            "slide 14": "Definition. An autonomous car is a vehicle capable of sensing its environment and operating without human involvement. An autonomous car can go anywhere a traditional car goes and do everything that an experienced human driver does. A picture of the interior of a self-driving car.",
            "slide 15": "6 levels of driving automation. The Society of Automotive Engineers (SAE) currently defines 6 levels of drving automation ranging from Level 0 (fully manual) to Level 5 (fully autonomous). Level 0-2: The human monitors the driving environment. Level 3-5: The automated system monitors the driving environment. A picture of the 6 levels of driving automation.",
            "slide 16": "Under the framework of reinforcement learning. Agent: The car or the computer inside the car. Environment: Many components. Traffic lights, pedestrian, other cars, road signs... State: Current location, green/red light, position of pedstrain, speed limit... Action: Brake, accelerate, turn left... Reward: No accident. A picture of an agent doing action to the environment and environment returns rewards and observations to the agent where the agent is the car and the environment is the world.",
            "slide 17": "How do autonumous cars work? Autonomous cars create and maintain a map of their surroundings based on a variety of sensors situated in different parts of the vehicle. Radar sensors monitor the position of nearby vehicles. Video cameras detect traffic lights, .etc. A picture of a radar. A picture of a camera.",
            "slide 18": "How do autonumous cars work? Lidar (light detection and ranging) sensors bounce pulses of light off the car’s surroundings to measure distances. Ultrasonic sensors in the wheels detect curbs and other vehicles when parking. A picture of lidar. A picture of Ultrasonic sensors.",
            "slide 19": "Challenges with autonomous cars. Lidar and Radar: Lidar is expensive. Multiple lidar signals may interfere with one another. Weather Conditions: How will the cameras and sensors track lane markings if the  markings are obscured by water, oil, ice, or debris? A picture of an icy road at night. A picture of a foggy road.",
            "slide 20": "Challenges with autonomous cars. Accident Liability: Who is liable for accidents caused by an autonomous car? Artificial vs. Emotional Intelligence: Human drivers rely on subtle cues and non-verbal communication to make split-second judgment calls and predict behaviors. A picture of a car crash.",
            "slide 21": "Benefits of autonomous cars. Reduce traffic congestion. Cut transportation costs by 40%. Improve walkability and livability. Free up parking lots for other uses. A picture of a self-driving taxi.",
            "slide 22": "Summary. The program can assess driving situations using cameras and other sensors. Methods: The program is trained using recorded scenes and stimulators. Pros and cons: 1. Autonomous cars maneuver in tricky situations. 2. Dealing with cars driven by humans. A picture of One of Google’s self-driving cars.",
            "slide 23": "RL lectures in a slide. A table with the following content (| indicates next column, > indicates next row). Type|Value of given state s|Condition>V-learning|r|The current reward is the only indicator of the value>V-learning|V_old + α * (r - V_old)|r is not consistent>V-learning|Neural network|# states is out of hand>Type|Q-value of given state s and action a|Condition>Q-learning|r|The current reward is the only indicator of the value>Q-learning|Q_old + α * (r - Q_old)|r is not consistent>Q-learning|Neural network|# states is out of hand",
            "slide 24": "Thanks!Q&A. Lecture 8."
        },
        "lecture 9": {
            "slide 1": "TALKING ARTIFICIAL NEURAL NETWORKS. Lecture 9",
            "slide 2": "Outline. How Do Voice-Controlled Personal Assistants Work?(highlighted). Speech Recognition. Sequence Processing. Task Execution.",
            "slide 3": "How Do Voice-Controlled Personal Assistants Work? We can ask for a joke to our voice-controlled personal assistant, and it seems to know what we’re talking about. Two pictures of Alexa, a personal assistant.",
            "slide 4": "How Do Voice-Controlled Personal Assistants Work? AI is behind those responses, tackling the very difficult tasks of understanding your speech, judging how to respond, and acting or answering using a natural-sounding voice. Voice recognition. Linguistic analysis. Conversational control. Database. Web content retrieval. A picture of a personal assistant.",
            "slide 5": "Outline. How Do Voice-Controlled Personal Assistants Work?. Speech Recognition (highlighted). Sequence Processing. Task Execution.",
            "slide 6": "Recognition Accuracy. We first need a system to recognize speech when creating a voice-controlled personal assistant. The accuracy of speech recognition is very demanding. 95% is not good. 99% is good. A picture of The Telephone Game to show the importance of speech recongition accuracy.",
            "slide 7": "ANN Has Contributed a Lot to Voice Recognition. A speech-recognition system does what our ears do: Converts speech sounds, A continuous change in air pressure, A continuous change in frequencies. A picture of a tuning fork.",
            "slide 8": "Speech Recognition. Here we talk about harmonies, a mixture of frequencies at each point. An ANN can translate frequencies into a series of linguistic sounds, such as /a/ /i/ /u/ /e/ /o/. Series of sounds then can be converted into words and phrases. A picture of a heatmap of frequency vs time.",
            "slide 9": "Outline. How Do Voice-Controlled Personal Assistants Work?. Speech Recognition. Sequence Processing (highlighted). Task Execution.",
            "slide 10": "How Should the Words be Represented in an ANN? Question: Why not type the words into a computer as a string of the alphabet for input? Assigning different neurons to different words. Then, add more information relating to each word: Represent a word with a group of words the original word is often used with. E.g. 'Cats' with 'meow' 'walk'. A picture of word cloud. A picture of a word connected to other words that it is related to.",
            "slide 11": "Words Representation. Method: finding out how often the word appears after which words and before which words by going through a huge data set. E.g. Words: {Cats, Dogs, Bark, Meow, Eat}. Phrases: {'Cats meow,' 'Dogs bark,' 'Cats eat,' 'Dogs eat'}. Result:'Cats' is often before 'meow' and 'eat', which can be expressed as (0,0,0,1,1). 'Cats' doesn’t often appear after any words, which can be expressed as (0,0,0,0,0). 'Cat” = (0,0,0,0,0, 0,0,0,1,1). A picture of a word cloud. A picture breaking down the grammar of an English sentence.",
            "slide 12": "More Models of Word Representation. Bag-of-Words Model: Taking neighboring words in a wider scope without considering the exact word order. Representation of words is called word embeddings. A picture showing the idea of word embedding where similar words have closer Euclidean distance.",
            "slide 13": "The Next Difficulty: Time. When an ANN works on a sentence, it needs to remember the previous words because the order is important. 'I am hungry' and 'Am I hungry' are different. The network remembers the word it has seen by sending the hidden layer activation back to the right input. A picture illustrating the last sentence.",
            "slide 14": "Recurrent Neural Network. The network is called a Recurrent Neural Network (RNN) because it has a loop in the connection to feed the previous activations back to the input. This network receives one word at a time and is trained to generate an output each time using supervised learning with additional 'desired outputs'. Another usage: They can also be trained to represent phrases and sentences. A picture of RNN.",
            "slide 15": "Training an RNN. Training an RNN by using backpropagation is more complicated than training an ANN with no loops. Unfolding: The loops are removed by duplicating parts of the neural network whenever they represent different parts of the sentence. After unfolding, we can train the RNN the way we train ANNs without loops. A picture illustrating the concept of unfolding.",
            "slide 16": "A Special Kind of RNN. This kind of RNN can encode a sequence of inputs to an activation pattern in a neural layer and to another sequence from the activity pattern. E.g. We can use this to translate an English sentence into a Spanish one. A picture showing that this kind of RNN can translate an English sentence into a Spanish sentence.",
            "slide 17": "A Special Kind of RNN. The pattern of activation after the RNN that reads the sequence can represent the sentence. 'Thought Vector': If the translation requires thinking about the sentence’s meaning, the representation of a sentence can be thought of as some aspect of the 'meaning' of the sentence. A picture of forward RNN and backward RNN.",
            "slide 18": "Outline. How Do Voice-Controlled Personal Assistants Work?. Speech Recognition. Sequence Processing. Task Execution (highlighted).",
            "slide 19": "Task Execution. The task of center unit (between speech recognition and production) is to translate the voice input into commands that assistant can execute. E.g. 'turning on a light', 'showing web search results'. If the command is very complicated, it sends a request through the Internet to a bigger computer system (server) for processing. A picture of a server.",
            "slide 20": "Voice Synthesis. Voice Synthesis is the final stage of a voice-controlled personal assistant. Speech synthesis is accomplished using an RNN that converts a series of linguistic sounds into a series of spectrum. A picture of chat history of a human with a bot.",
            "slide 21": "Summary. We’ve looked at a voice-controlled personal assistant, which is just one example of an AI-powered natural language processing application. A chatbot is another interesting type of application that shows how AI-based natural language processing can be used. Currently, chat bots are used for entertainment, customer support, online shopping, and so on. Two pictures of chatbots.",
            "slide 22": "Q&A. Lecture 9.",
            "slide 23": "How Do Voice-Controlled Personal Assistants Work? We can ask for a joke to our voice-controlled personal assistant, and it seems to know what we're talking about. A picture of a voice-controlled personal assistant. A picture of a phone.",
            "slide 24": "Recognition Accuracy. We first need a system to recognize speech when creating a voice-controlled personal assistant. The accuracy of speech recognition is very demanding. 95% is not good. 99% is good. Note: If the speech recognition rate improves to 99%, speech input will become the primary mode of communicating with computers. A picture of a mic on top of a phone.",
            "slide 25": "ANN Has Contributed a Lot to Voice Recognition. A speech-recognition system does what our ears do: Converts speech sounds, A continuous change in air pressure, A continuous change in frequencies. A picture of a hearing aid. A picture of the data a mic receieves.",
            "slide 26": "Recurrent Neural Network. The network is called a Recurrent Neural Network (RNN) because it has a loop in the connection to feed the previous activations back to the input. This network receives one word at a time and is trained to generate an output each time using supervised learning with additional 'desired outputs'. Another usage: They can also be trained to represent phrases and sentences. A picture of a RNN."     
        },
        "lecture 10": {
            "slide 1": "SEEING ARTIFICIAL NEURAL NETWORKS. Lecture 10.",
            "slide 2": "Outline. Artificial neural networks can see the world (highlighted). What they see. How they understand: CNN.",
            "slide 3": "Health Industry. AI doctors help to locate tumor regions. Magnetic resonance images are processed by computer and helps doctors to classify the status and type of the tumor in brain. A picture of a brain MRI image.",
            "slide 4": "Facial recognition system. Computers/mobile devices are able to tell who you are. Windows Hello from Microsoft or Face ID from Apple are two commercial examples. A picture of a laptop with Windows Hello.",
            "slide 5": "Outline. Artificial neural networks can see the world. What they see (highlighted). How they understand: CNN.",
            "slide 6": "Videos and images. A video comprises several images, which is called frame. A picture of a flip book. A picture illstrating the concept of a frame.",
            "slide 7": "Videos and images. Commonly, 24 frames are shown within a second, which is termed as frame rate and the unit is frames per second (fps). A video showing the difference between different fps. A picture illstrating the concept of a frame.",
            "slide 8": "Images are stored as pixels. When zooming in a picture real close, we find it is composed of mosaics. These small squares are pixels. We fill each pixel with color and then we get a picture! A picture showing the effect of zooming in a picture will get mosaics or pixels.",
            "slide 9": "What exactly is stored in computer? Red, green and blue are three primary colors. When blend together, they can generate a full range of colors. The computer actually stores the colors of each pixel, and in our case, the value of red, green and blue. A picture showing RGB are the primary colors. A picture of a traffic sign and its corresponding pixel values representation.",
            "slide 10": "Outline. Artificial neural networks can see the world. What they see. How they understand: CNN (highlighted).",
            "slide 11": "The jigsaw puzzle. Computers assemble visual images in the same way you might put together a jigsaw puzzle. Computers distinguish many different pieces of the image, they identify the edges and then model the subcomponents. Using filtering and a series of actions through deep network layers, they can piece all the parts of the image together, much like you would with a puzzle. A picture of a jigsaw puzzle.",
            "slide 12": "Neural networks (CNN) learn from labelled images. The CIFAR-10 and CIFAR-100 are labeled subsets of the 80 million tiny images dataset. They were collected by Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. A picture showing labelled images as inputs to a CNN to let the CNN learn what are planes and what are automobiles.",
            "slide 13": "Convolutional neural networks components. Convolution layers. RELU. Pooling layers. A picture of different layers of a classic CNN",
            "slide 14": "Convolution layer. The convolution layer is the core building block of the CNN. This layer performs a dot product (one-to-one) between two matrices (ordered numbers). One is the image, and the other is called kernel, mimicing the way human visual system works. A picture showing the computation process of convolution.",
            "slide 15": "Convolution layer in animation and why we convolutes?Dr. Hubel and Dr. Wiesel found that some neurons fired very rapidly by watching the lines at specific angles, while other neurons responded best to the lines at different angles. The kernels (smaller matrix) are similar to the neurons: some produce larger output when given certain features like horizontal edges, while some respond to vertical edges. An animation of convolution computation.",
            "slide 16": "ReLU, a math transformation. Rectified Linear Unit. output = input if input > 0, 0 otherwise. output = max(input, 0). Mimics the way how biological neurons are triggered! A picture of different numbers and their output after ReLU.",
            "slide 17": "Pooling layer. Pooling condense the output of small regions of neurons into a single output. This helps simplify the following layers and reduces the number of parameters that the model needs to learn. Max pooling: take the largest number (maximum) in the region. Average pooling: take the average of the numbers in the region. A picture showing max pooling and average pooling.",
            "slide 18": "Convolutional neural networks. A CNN can have tens or hundreds of hidden layers that each learn to detect different features in an image. Classification part produces the final prediction of what is in the image using the information gathered from hidden layers. A picture of different layers of a classical CNN.",
            "slide 19": "AlexNet: A CNN model follows previous pattern. AlexNet won the 2012 ImageNet competition, where computers should tell what is in the picture, with a top-5 error rate of 15.3%, compared to the second place top-5 error rate of 26.2%. top-5 error: predicting 5 categories and if all are wrong, it counts as an error. A picture of AlexNet architecture. A bar graph of classification error rate vs different method groups",
            "slide 20": "THANK YOU! Q&A. Lecture 10."
        },
        "lecture 11": {
            "slide 1": "Artificial and Biological Neural Networks.Lecture 11.",
            "slide 2": "Outline. Why should we study the brain? (highlighted). Areas where ANN can learn from humans. Areas where ANN can learn from the brain. Brain-machine interface.",
            "slide 3": "Introduction. ANNs were originally inspired by the human brain. Many functions of an ANN were modeled after biological neurons. Biological neurons at an abstract level: Multiplie the inputs with the connection weights, Accumulate the results, Apply output function. A picture of a neuron.",
            "slide 4": "The interaction has decreased. A real neuron is made of tiny electrochemical processes that are only roughly approximated in an artificial neuron model. It is unlikely that backpropagation is going on in the brain when people learn, while backpropagation is a powerful algorithm for training ANNs. A picture of backpropagation.",
            "slide 5": "We should watch for developments in neuroscience. There are still areas where people perform much better than AI. There may be a different operating principle we can learn from biological neural networks. Two pictures that show the similarty between a biological neuron and the one in ANN.",
            "slide 6": "Outline. Why should we study the brain?. Areas where ANN can learn from humans (highlighted). Areas where ANN can learn from the brain. Brain-machine interface.",
            "slide 7": "Areas where ANN can Learn from Humans. The human brain works better than ANN in some ways. For example, humans, even children, learn faster than ANN. People require fewer examples to learn a concept and people also tend to make less mistakes than do ANN. A picture of a word cloud about learning.",
            "slide 8": "Graphical model. Graphical model is a new kind of fast-learning algorithm. Nodes: describe anything that can be represented by numbers or labels. Arrows and lines: show one node’s value ends up with the given value of another node. A picture of an example graph.",
            "slide 9": "Graphical model. Five variables: difficulty, intelligence, grade, SAT, letter. Relationship: The difficulty of the course and the intelligence of the student affect the student’s grade. The SAT score is only influenced by the student’s intelligence. The grade affects whether the student gets a good recommendation letter. A picture of a graph representing this relationship.",
            "slide 10": "Few-shot learning. Few-shot learning aims to build accurate machine learning models with less training data. A picture highlighting the difference between large data training and few-shot training.",
            "slide 11": "Generative model. Another thing that humans have but machines seem to lack is creativity. ANN models called the generative model can create examples after learning. Generative adversarial network (GAN) learns to generate outputs indistinguishable from the real data. Images generated by GAN.",
            "slide 12": "GAN in details. A picture of the GAN architecture (random noise to generator to generate a fake image and a discriminator trying to discriminate that from the training image)",
            "slide 13": "Transfer learning to new areas. Humans also seem to have the ability to transfer learning to new areas. A network trained for one task may be able to be trained for another task faster if the original task and the applied task have some common features. A picture showing transfer learning where the source data/labels are larger than the target data/labels.",
            "slide 14": "Transfer learning in CNNs. ConvNet as pre-trained model for new tasks! A picture of the architecture of a classical CNN.",
            "slide 15": "Outline. Why should we study the brain?. Areas where ANN can learn from humans. Areas where ANN can learn from the brain (highlighted). Brain-machine interface.",
            "slide 16": "Computational neuroscience. Computational neuroscience examines how the small components of neurons and synapses work, and then analyzes in detail the raw signals the neurons create. Computational neuroscientists record from a single cell or put sensors in the brains of mice. Researchers also run computer simulations of minute process within a neuron. A picture of a computer simulation of the activities of a small circuit in the rat brain.",
            "slide 17": "The structure of human brain. The “big brain” of humans is made of six layers organized into columns. We don’t precisely understand how the layers and columns work although we know how these cell types are distributed in the layers. Once we find out, the understanding of brain may change and impact ANN technology. A picture showing the six layers of a human brain.",
            "slide 18": "Human brainwaves. Humans and many animals have rhythmic electrical activities in the brain called brainwaves. Big brainwaves can be recorded using an EEG, which involves putting electrodes on the skin of the head and recording voltages. A picture of an EEG.",
            "slide 19": "Human brainwaves. Brainwaves are an important part of information processing in the brain. E.g. we may predict what the mouse will do by recording the brainwaves. Most ANN models don’t generate brainwaves, which means that the real brain has some different operating principles. A picture of a brainwave.",
            "slide 20": "Summary. It may take a long time before the research of brain works will be relevant to ANNs. It’s up to future AI researchers and developers to decide whether ANNs can incorporate some of the findings and ideas from neuroscience. A picture of a brain.",
            "slide 21": "Outline. Why should we study the brain?. Areas where ANN can learn from humans. Areas where ANN can learn from the brain. Brain-machine interface (highlighted).",
            "slide 22": "Brain-machine interface. We can determine what a person has decided in his or her mind by reading brain images. From neurons to machines: We can recognize the decision from measured activities and how to control machines use that knowledge. A picture of a man controlling a robot car with his brainwaves.",
            "slide 23": "Brain-machine interface in the opposite direction. From machines to neurons: People who can’t hear can sense sounds by connecting a sound analyzer. People who are blind can sense brightness and shapes by connecting light sensors to sensory neurons. A picture of a man controlling a robot car with his brainwaves.",
            "slide 24": "Summary. Why should we study the brain?. Areas where ANN can learn from humans. Areas where ANN can learn from the brain. Brain-machine interface.",
            "slide 25": "Thanks! Q&A. Lecture 11."
        },
        "lecture 12": {
            "slide 1": "ARTIFICIAL INTELLIGENCE: TODAY AND TOMORROW. LECTURE 12",
            "slide 2": "Outline. Cutting edge (highlighted). Future. Proceed with care.",
            "slide 3": "Usage of AI. Image of a voice-controlled personal assistant. Image of a self-driving car. Image of a brain MRI.",
            "slide 4": "Medicine. finding new drugs. medical diagnoses. With the vast knowledge of paintents , and chemical sturctures sotred in databases, AI system can search for potentially effective treatments more quickly than human experts can. A video about AI usage in medicine.",
            "slide 5": "Natural language application. machine translation. natural language interfaces. AI systems extract knowledge from natural language texts. E.g., Q: The headquarters of BHP Billiton Limited, and the global headquarters of the combined BHP Billiton Group, are located in Melbourne, Australia. A: headquarters('“'BHP Biliton Limited', 'Melbourne, Australia') A cartoon picture showing that.",
            "slide 6": "Taking care of us. Robots that take care of patients in general are in development. A picture of an old lady holding the hand of a robot. Robot as a friend. Robot “reads” your emotions and then try to make you feel better. A picture of a robot. ",
            "slide 7": "Usage in dangerous situations. Situations human want to avoid: Old minefields, automobile factories, other planets... AI can make machines in these situations more autonomous and useful. A picture of Curious, a robot that was sent to Mars.",
            "slide 8": "Summary. The real-world aplication using AI have just begun, and AI technology hasn’t fully matured and been tested. AI-related news should be read with caution. A picture of a robot.",
            "slide 9": "Outline. Cutting edge. Future (highlighted). Proceed with care.",
            "slide 10": "Integrate with our brains. We expect our brains will be able to communicate with AI through implanted electrodes to expand out cognitive capabilities. Development in this direction may take many years because we must do more research on the human brain side. A picture of a human brain but it's digital.",
            "slide 11": "Solve some of the gravest human problems. poverty, hunger, climate change. Global problems require fast global data acquisition, statistical analysis, knowledge, and strategic thinking, which AI will continue to improve on. A picture of giant machines.",
            "slide 12": "Supporting hardware. Spiking neural networks: They consider the spike timings rather than just the spiking frequency, which is an important feature of biological neural networls. A biological neuron integrates the inputs and decides whether it generates a spike or not. Three images showing that.",
            "slide 13": "Quantum computer. Quantum computers will be faster and smaller. Traditional: Bits. Quantum: Qubits. Represent numbers at the same time. In situations where there are a large number of possible combinations, quantum computers can consider them simultaneously. A picture of a futuristic chip.",
            "slide 14": "Artificial general intelligence (AGI). AGI is a generic, human-like intelligence that can tackle problems by developing and applying general congnitive abilities across different areas. Many people think that the progress in AI will be made only by tackling specific problems, not by trying to advance machine intelligence in general. A picture of a robot trying to solve complex math/physics problems.",
            "slide 15": "Artificial general intelligence (AGI). A machine use millions of cameras or microphones that it isn't limited by human vision or hearing. Machines can integrate all the domain-specific knowledge and add a module that has human-like intuitions as a legacy component. The world’s top-level researchers are working to develop AGI. A picture of a brain floating on a chip.",
            "slide 16": "Outline. Cutting edge. Future. Proceed with care (highlighted).",
            "slide 17": "Safety issues. The ANN components of AI can drive a car very well but can’t explain how it’s doing the job. If something goes wrong, who is to blame? The developer of the automobiler? The develooper of the AI module? A picture of a futuristic car.",
            "slide 18": "Ethical issues. If AI are developed with more sensitive sensors and more autonomous control, will robots, at some point, start to have personalities and consciousness? Should we worry about respecting their opinions or their will for existence? A picture of 2 robots.",
            "slide 19": "AI may spell the end to human intelligence. Some of the world’s most intelligent people have warned that AI may spell the end to human intelligence. When AI is more intelligent than we are, we can lose control of it, and we can no longer guatantee that AI will take actions we think of as good. A picture of Stephen Hawking, Elon Musk, and Bill Gates.",
            "slide 20": "Unemployment problems. We already have unemployment problems in many parts of the world. There is no doubt that AI is already replacing some jobs. Sooner or later, AI will surpass human beings in terms of intelligence. Generative ANNs can be trained to create samples of art that it hasn’t seen before. A picture of a robot as a coworker of 2 humans.",
            "slide 21": "Summary. When AI suipass human intelligence, our worth may not be measured by how smart we are, but how wise we are. We should be samrt enough to use AI and wise enough to plan our future so that using AI will enrich our lives. Maybe we should stop developing AI, or maybe we should rest our future in the hands of AI. What do you think? A picture of a child making a robot.",
            "slide 22": "Q&A. Thanks! Lecture 12"
        }
    }
}